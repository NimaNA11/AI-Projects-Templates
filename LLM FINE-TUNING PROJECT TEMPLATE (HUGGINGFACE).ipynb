{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM FINE-TUNING PROJECT TEMPLATE (HUGGINGFACE)\n",
    "==============================================\n",
    "Use Case: Domain-specific LLM, Instruction Tuning, Task-specific Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PROJECT SETUP & ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets accelerate peft bitsandbytes\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install wandb tensorboard\n",
    "# !pip install sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HuggingFace\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "from evaluate import load as load_metric\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Model configuration\n",
    "    'base_model': 'meta-llama/Llama-2-7b-hf',  # or 'mistralai/Mistral-7B-v0.1', 'gpt2', etc.\n",
    "    'model_type': 'causal',  # 'causal' or 'seq2seq'\n",
    "    \n",
    "    # Data configuration\n",
    "    'dataset_name': 'custom',  # or HF dataset like 'squad', 'alpaca', etc.\n",
    "    'train_file': 'train.json',\n",
    "    'val_file': 'val.json',\n",
    "    'max_length': 512,\n",
    "    'test_size': 0.1,\n",
    "    \n",
    "    # Training configuration\n",
    "    'output_dir': './finetuned_model',\n",
    "    'num_epochs': 3,\n",
    "    'batch_size': 4,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'learning_rate': 2e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 100,\n",
    "    'logging_steps': 10,\n",
    "    'save_steps': 500,\n",
    "    'eval_steps': 500,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'fp16': True,  # Mixed precision training\n",
    "    \n",
    "    # LoRA configuration\n",
    "    'use_lora': True,\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0.05,\n",
    "    'lora_target_modules': ['q_proj', 'v_proj'],\n",
    "    \n",
    "    # Generation configuration\n",
    "    'max_new_tokens': 256,\n",
    "    'temperature': 0.7,\n",
    "    'top_p': 0.9,\n",
    "    'top_k': 50,\n",
    "    \n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "np.random.seed(CONFIG['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA LOADING & PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_dataset(train_file, val_file=None):\n",
    "    \"\"\"Load custom dataset from JSON files\"\"\"\n",
    "    with open(train_file, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    \n",
    "    if val_file:\n",
    "        with open(val_file, 'r') as f:\n",
    "            val_data = json.load(f)\n",
    "    else:\n",
    "        # Split train data\n",
    "        split_idx = int(len(train_data) * (1 - CONFIG['test_size']))\n",
    "        val_data = train_data[split_idx:]\n",
    "        train_data = train_data[:split_idx]\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# Load data\n",
    "if CONFIG['dataset_name'] == 'custom':\n",
    "    train_data, val_data = load_custom_dataset(CONFIG['train_file'], CONFIG['val_file'])\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    dataset = DatasetDict({\n",
    "        'train': Dataset.from_list(train_data),\n",
    "        'validation': Dataset.from_list(val_data)\n",
    "    })\n",
    "else:\n",
    "    # Load from HuggingFace Hub\n",
    "    dataset = load_dataset(CONFIG['dataset_name'])\n",
    "\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Validation size: {len(dataset['validation'])}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "def analyze_text_lengths(dataset, text_field='text'):\n",
    "    \"\"\"Analyze distribution of text lengths\"\"\"\n",
    "    lengths = [len(sample[text_field].split()) for sample in dataset['train']]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(lengths, bins=50, edgecolor='black')\n",
    "    plt.xlabel('Number of Tokens')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Text Lengths')\n",
    "    plt.axvline(x=np.mean(lengths), color='r', linestyle='--', label=f'Mean: {np.mean(lengths):.0f}')\n",
    "    plt.axvline(x=np.percentile(lengths, 95), color='g', linestyle='--', label=f'95th percentile: {np.percentile(lengths, 95):.0f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(lengths)\n",
    "    plt.ylabel('Number of Tokens')\n",
    "    plt.title('Box Plot of Text Lengths')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Mean length: {np.mean(lengths):.2f} tokens\")\n",
    "    print(f\"Median length: {np.median(lengths):.2f} tokens\")\n",
    "    print(f\"95th percentile: {np.percentile(lengths, 95):.2f} tokens\")\n",
    "\n",
    "# Uncomment based on your data structure\n",
    "# analyze_text_lengths(dataset, text_field='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MODEL & TOKENIZER LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG['base_model'],\n",
    "    trust_remote_code=True,\n",
    "    padding_side='right'  # Important for generation\n",
    ")\n",
    "\n",
    "# Add special tokens if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer loaded: {CONFIG['base_model']}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with optimizations\n",
    "if CONFIG['use_lora']:\n",
    "    # Load in 8-bit for efficient fine-tuning\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG['base_model'],\n",
    "        load_in_8bit=True,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Prepare for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG['base_model'],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "print(f\"Model loaded: {CONFIG['base_model']}\")\n",
    "print(f\"Parameters: {model.num_parameters() / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Configure LoRA (Parameter-Efficient Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['use_lora']:\n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=CONFIG['lora_r'],\n",
    "        lora_alpha=CONFIG['lora_alpha'],\n",
    "        lora_dropout=CONFIG['lora_dropout'],\n",
    "        target_modules=CONFIG['lora_target_modules'],\n",
    "        bias='none',\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    # Wrap model with LoRA\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DATA PREPROCESSING & TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Instruction Format (Alpaca-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format sample in instruction-following format\"\"\"\n",
    "    if 'instruction' in sample and 'output' in sample:\n",
    "        if 'input' in sample and sample['input']:\n",
    "            prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\"\"\"\n",
    "    else:\n",
    "        # For conversational data\n",
    "        prompt = sample.get('text', '')\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize examples for training\"\"\"\n",
    "    # Format as instructions\n",
    "    if 'instruction' in examples:\n",
    "        texts = [format_instruction(ex) for ex in examples]\n",
    "    else:\n",
    "        texts = examples['text']\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "print(f\"Sample tokenized data:\")\n",
    "print(tokenized_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM (not masked LM)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TRAINING CONFIGURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    warmup_steps=CONFIG['warmup_steps'],\n",
    "    max_grad_norm=CONFIG['max_grad_norm'],\n",
    "    \n",
    "    # Optimization\n",
    "    fp16=CONFIG['fp16'],\n",
    "    optim='adamw_torch',\n",
    "    lr_scheduler_type='cosine',\n",
    "    \n",
    "    # Logging & Evaluation\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_steps=CONFIG['logging_steps'],\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=CONFIG['eval_steps'],\n",
    "    save_strategy='steps',\n",
    "    save_steps=CONFIG['save_steps'],\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Other\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    report_to='tensorboard',\n",
    "    seed=CONFIG['random_seed'],\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # For generation tasks, this is simplified\n",
    "    # In practice, you'd use BLEU, ROUGE, etc.\n",
    "    \n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Mask padding tokens\n",
    "    mask = labels != -100\n",
    "    predictions = predictions[mask]\n",
    "    labels = labels[mask]\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(labels.flatten(), predictions.flatten())\n",
    "    \n",
    "    return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training time: {metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples per second: {metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract loss values\n",
    "train_loss = [log['loss'] for log in log_history if 'loss' in log]\n",
    "eval_loss = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "steps_train = [log['step'] for log in log_history if 'loss' in log]\n",
    "steps_eval = [log['step'] for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(steps_train, train_loss, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(steps_eval, eval_loss, label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Steps')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Progress')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_values = [log['learning_rate'] for log in log_history if 'learning_rate' in log]\n",
    "lr_steps = [log['step'] for log in log_history if 'learning_rate' in log]\n",
    "\n",
    "axes[1].plot(lr_steps, lr_values, linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Steps')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Generation Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for text generation metrics\n",
    "# !pip install rouge-score bert-score\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "# from bert_score import score as bert_score\n",
    "\n",
    "def evaluate_generation(model, tokenizer, test_samples, max_new_tokens=256):\n",
    "    \"\"\"Evaluate generation quality\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for sample in test_samples:\n",
    "        # Get instruction\n",
    "        if 'instruction' in sample:\n",
    "            instruction = sample['instruction']\n",
    "            expected_output = sample['output']\n",
    "            \n",
    "            prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "        else:\n",
    "            prompt = sample.get('prompt', sample.get('text', ''))\n",
    "            expected_output = sample.get('completion', sample.get('output', ''))\n",
    "        \n",
    "        # Generate\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=CONFIG['temperature'],\n",
    "                top_p=CONFIG['top_p'],\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the response part\n",
    "        if \"### Response:\" in generated_text:\n",
    "            generated_text = generated_text.split(\"### Response:\")[-1].strip()\n",
    "        \n",
    "        predictions.append(generated_text)\n",
    "        references.append(expected_output)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key].append(scores[key].fmeasure)\n",
    "    \n",
    "    # Average scores\n",
    "    avg_scores = {key: np.mean(values) for key, values in rouge_scores.items()}\n",
    "    \n",
    "    return avg_scores, predictions, references\n",
    "\n",
    "# Evaluate on a subset\n",
    "test_samples = dataset['validation'].select(range(min(10, len(dataset['validation']))))\n",
    "rouge_scores, predictions, references = evaluate_generation(model, tokenizer, test_samples)\n",
    "\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for metric, score in rouge_scores.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (pred, ref) in enumerate(zip(predictions[:3], references[:3]), 1):\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(f\"Expected: {ref[:200]}...\")\n",
    "    print(f\"\\nGenerated: {pred[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. INFERENCE & GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_new_tokens=256):\n",
    "    \"\"\"Generate response for a given prompt\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Format prompt\n",
    "    formatted_prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=CONFIG['temperature'],\n",
    "            top_p=CONFIG['top_p'],\n",
    "            top_k=CONFIG['top_k'],\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract response\n",
    "    if \"### Response:\" in generated_text:\n",
    "        response = generated_text.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        response = generated_text\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom prompts\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain the concept of neural networks.\",\n",
    "    \"How does fine-tuning work?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERACTIVE GENERATION TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = generate_response(prompt, model, tokenizer)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate(prompts, model, tokenizer, max_new_tokens=256, batch_size=4):\n",
    "    \"\"\"Generate responses for multiple prompts\"\"\"\n",
    "    model.eval()\n",
    "    all_responses = []\n",
    "    \n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        \n",
    "        # Format prompts\n",
    "        formatted_prompts = [\n",
    "            f\"### Instruction:\\n{p}\\n\\n### Response:\\n\" \n",
    "            for p in batch_prompts\n",
    "        ]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=CONFIG['max_length']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=CONFIG['temperature'],\n",
    "                top_p=CONFIG['top_p'],\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        all_responses.extend(responses)\n",
    "    \n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. MODEL SAVING & EXPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_path = f\"{CONFIG['output_dir']}/final_model\"\n",
    "trainer.save_model(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(f\"Model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Save LoRA Adapters Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['use_lora']:\n",
    "    # Save only LoRA adapters (much smaller)\n",
    "    lora_path = f\"{CONFIG['output_dir']}/lora_adapters\"\n",
    "    model.save_pretrained(lora_path)\n",
    "    print(f\"LoRA adapters saved to: {lora_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Merge LoRA with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['use_lora']:\n",
    "    # Merge LoRA weights with base model\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    merged_path = f\"{CONFIG['output_dir']}/merged_model\"\n",
    "    merged_model.save_pretrained(merged_path)\n",
    "    tokenizer.save_pretrained(merged_path)\n",
    "    \n",
    "    print(f\"Merged model saved to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Export to GGUF (for llama.cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For deployment with llama.cpp\n",
    "# Install: pip install gguf\n",
    "# Then convert using: python convert-hf-to-gguf.py <model_path>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. MODEL LOADING & DEPLOYMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_path):\n",
    "    \"\"\"Load the fine-tuned model for inference\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model\n",
    "# loaded_model, loaded_tokenizer = load_finetuned_model(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 API Deployment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMInferenceAPI:\n",
    "    \"\"\"Production-ready inference API\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model, self.tokenizer = load_finetuned_model(model_path)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def generate(self, prompt, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
    "        \"\"\"Generate response for a prompt\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    \n",
    "    def chat(self, messages, max_new_tokens=256):\n",
    "        \"\"\"Multi-turn conversation\"\"\"\n",
    "        # Format conversation history\n",
    "        conversation = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg['role']\n",
    "            content = msg['content']\n",
    "            conversation += f\"{role}: {content}\\n\"\n",
    "        \n",
    "        conversation += \"Assistant: \"\n",
    "        \n",
    "        return self.generate(conversation, max_new_tokens)\n",
    "\n",
    "# Initialize API\n",
    "# api = LLMInferenceAPI(output_path)\n",
    "# response = api.generate(\"Your prompt here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. ADVANCED TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 QLoRA (Quantized LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 4-bit quantization training\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "# model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "#     CONFIG['base_model'],\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map='auto'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Instruction Tuning with RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reinforcement learning from human feedback\n",
    "# Install: pip install trl\n",
    "\n",
    "# from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "# from trl.core import LengthSampler\n",
    "\n",
    "# # Configure PPO\n",
    "# ppo_config = PPOConfig(\n",
    "#     model_name=CONFIG['base_model'],\n",
    "#     learning_rate=1.41e-5,\n",
    "#     batch_size=16,\n",
    "#     mini_batch_size=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Multi-GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For distributed training across multiple GPUs\n",
    "# Use accelerate config and modify training_args:\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     ...\n",
    "#     deepspeed='ds_config.json',  # DeepSpeed configuration\n",
    "#     fsdp='full_shard auto_wrap',  # Fully Sharded Data Parallel\n",
    "#     ...\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. MONITORING & ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Model Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_size(model):\n",
    "    \"\"\"Analyze model parameter counts\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "    print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    # Memory footprint\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    print(f\"Model size: {size_mb:.2f} MB\")\n",
    "\n",
    "analyze_model_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Inference Speed Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_inference(model, tokenizer, num_samples=10):\n",
    "    \"\"\"Benchmark inference speed\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_prompt = \"What is artificial intelligence?\"\n",
    "    inputs = tokenizer(test_prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_new_tokens=50)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_samples):\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_new_tokens=50)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    print(f\"Average inference time: {avg_time:.3f}s Â± {std_time:.3f}s\")\n",
    "    print(f\"Tokens per second: {50/avg_time:.2f}\")\n",
    "\n",
    "benchmark_inference(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. CONCLUSIONS & NEXT STEPS\n",
    "\n",
    "## Summary:\n",
    "- Base Model: {CONFIG['base_model']}\n",
    "- Fine-tuning Method: {'LoRA' if CONFIG['use_lora'] else 'Full Fine-tuning'}\n",
    "- Training Samples: {len(dataset['train'])}\n",
    "- Final Training Loss: X.XX\n",
    "- Final Validation Loss: X.XX\n",
    "\n",
    "## Next Steps:\n",
    "- [ ] Experiment with different LoRA ranks\n",
    "- [ ] Try instruction tuning with more diverse data\n",
    "- [ ] Implement RLHF for alignment\n",
    "- [ ] Quantize model for deployment (GPTQ, AWQ)\n",
    "- [ ] Deploy with vLLM or TensorRT-LLM\n",
    "- [ ] Create a chatbot interface with Gradio/Streamlit\n",
    "- [ ] Monitor and collect user feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
