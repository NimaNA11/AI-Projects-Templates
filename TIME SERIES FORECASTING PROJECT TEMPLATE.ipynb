{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME SERIES FORECASTING PROJECT TEMPLATE\n",
    "=========================================\n",
    "Use Case: Stock Prediction, Energy Demand, Sales Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PROJECT SETUP & ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical models\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_path': 'timeseries_data.csv',\n",
    "    'date_column': 'date',\n",
    "    'target_column': 'value',\n",
    "    'lookback_window': 30,\n",
    "    'forecast_horizon': 7,\n",
    "    'train_split': 0.8,\n",
    "    'val_split': 0.1,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 100,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.2,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "torch.manual_seed(CONFIG['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA LOADING & EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load time series data\n",
    "df = pd.read_csv(CONFIG['data_path'], parse_dates=[CONFIG['date_column']])\n",
    "df = df.sort_values(CONFIG['date_column'])\n",
    "df.set_index(CONFIG['date_column'], inplace=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Check data frequency\n",
    "print(f\"Data frequency: {pd.infer_freq(df.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the time series\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df.index, df[CONFIG['target_column']], linewidth=1)\n",
    "plt.title('Time Series Data', fontsize=16)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(CONFIG['target_column'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df[CONFIG['target_column']], bins=50, edgecolor='black')\n",
    "axes[0].set_title('Distribution of Values')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df[CONFIG['target_column']])\n",
    "axes[1].set_title('Box Plot')\n",
    "axes[1].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TIME SERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Stationarity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series):\n",
    "    \"\"\"Augmented Dickey-Fuller test for stationarity\"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    \n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"\\nResult: Series is stationary\")\n",
    "    else:\n",
    "        print(\"\\nResult: Series is non-stationary\")\n",
    "    \n",
    "    return result[1] <= 0.05\n",
    "\n",
    "is_stationary = adf_test(df[CONFIG['target_column']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal decomposition\n",
    "decomposition = seasonal_decompose(\n",
    "    df[CONFIG['target_column']], \n",
    "    model='additive',\n",
    "    period=min(365, len(df)//2)  # Adjust based on your data\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "axes[0].plot(df.index, df[CONFIG['target_column']])\n",
    "axes[0].set_title('Original Time Series')\n",
    "axes[0].set_ylabel('Value')\n",
    "\n",
    "axes[1].plot(df.index, decomposition.trend)\n",
    "axes[1].set_title('Trend Component')\n",
    "axes[1].set_ylabel('Trend')\n",
    "\n",
    "axes[2].plot(df.index, decomposition.seasonal)\n",
    "axes[2].set_title('Seasonal Component')\n",
    "axes[2].set_ylabel('Seasonal')\n",
    "\n",
    "axes[3].plot(df.index, decomposition.resid)\n",
    "axes[3].set_title('Residual Component')\n",
    "axes[3].set_ylabel('Residual')\n",
    "axes[3].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ACF and PACF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# ACF plot\n",
    "plot_acf(df[CONFIG['target_column']].dropna(), lags=40, ax=axes[0])\n",
    "axes[0].set_title('Autocorrelation Function (ACF)')\n",
    "\n",
    "# PACF plot\n",
    "plot_pacf(df[CONFIG['target_column']].dropna(), lags=40, ax=axes[1])\n",
    "axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill missing values\n",
    "df_filled = df.fillna(method='ffill')\n",
    "\n",
    "# Alternative: interpolation\n",
    "# df_filled = df.interpolate(method='linear')\n",
    "\n",
    "print(f\"Missing values after filling: {df_filled.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, target_col):\n",
    "    \"\"\"Create time-based features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day'] = df.index.day\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 7, 30]:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for window in [7, 30]:\n",
    "        df[f'rolling_mean_{window}'] = df[target_col].rolling(window=window).mean()\n",
    "        df[f'rolling_std_{window}'] = df[target_col].rolling(window=window).std()\n",
    "    \n",
    "    # Difference features\n",
    "    df['diff_1'] = df[target_col].diff(1)\n",
    "    df['diff_7'] = df[target_col].diff(7)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_features = create_features(df_filled, CONFIG['target_column'])\n",
    "df_features.dropna(inplace=True)\n",
    "\n",
    "print(f\"Features created: {df_features.shape[1]} columns\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df_filled[[CONFIG['target_column']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SEQUENCE CREATION FOR DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, lookback, forecast_horizon):\n",
    "    \"\"\"Create input sequences and targets\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - lookback - forecast_horizon + 1):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback:i+lookback+forecast_horizon])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(scaled_data, CONFIG['lookback_window'], CONFIG['forecast_horizon'])\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")  # (samples, lookback, features)\n",
    "print(f\"Output shape: {y.shape}\")  # (samples, forecast_horizon, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Train-Val-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X) * CONFIG['train_split'])\n",
    "val_size = int(len(X) * CONFIG['val_split'])\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_val = X[train_size:train_size+val_size]\n",
    "y_val = y[train_size:train_size+val_size]\n",
    "\n",
    "X_test = X[train_size+val_size:]\n",
    "y_test = y[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. STATISTICAL MODELS (BASELINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ARIMA model\n",
    "train_data = df_filled[CONFIG['target_column']][:train_size]\n",
    "\n",
    "arima_model = ARIMA(train_data, order=(5, 1, 2))  # (p, d, q)\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "print(arima_result.summary())\n",
    "\n",
    "# Make predictions\n",
    "arima_forecast = arima_result.forecast(steps=len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "es_model = ExponentialSmoothing(\n",
    "    train_data,\n",
    "    seasonal_periods=12,\n",
    "    trend='add',\n",
    "    seasonal='add'\n",
    ")\n",
    "es_result = es_model.fit()\n",
    "\n",
    "es_forecast = es_result.forecast(steps=len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. DEEP LEARNING MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take the last output\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel(\n",
    "    input_size=1,\n",
    "    hidden_size=CONFIG['hidden_size'],\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    output_size=CONFIG['forecast_horizon'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 GRU Model (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Transformer Model (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, output_size, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.transformer(x)\n",
    "        out = self.fc(x[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).squeeze().to(device)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).squeeze().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, epochs):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.6f}, Val Loss: {val_loss.item():.6f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model, \n",
    "    X_train_tensor, \n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    CONFIG['num_epochs']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Make predictions\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# Inverse transform\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "predictions_actual = scaler.inverse_transform(predictions.reshape(-1, 1)).reshape(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate forecasting metrics\"\"\"\n",
    "    mse = mean_squared_error(y_true.flatten(), y_pred.flatten())\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "    mape = np.mean(np.abs((y_true.flatten() - y_pred.flatten()) / y_true.flatten())) * 100\n",
    "    r2 = r2_score(y_true.flatten(), y_pred.flatten())\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    }\n",
    "\n",
    "metrics = calculate_metrics(y_test_actual, predictions_actual)\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for a sample\n",
    "sample_idx = 0\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(range(CONFIG['forecast_horizon']), y_test_actual[sample_idx], \n",
    "         label='Actual', marker='o', linewidth=2)\n",
    "plt.plot(range(CONFIG['forecast_horizon']), predictions_actual[sample_idx], \n",
    "         label='Predicted', marker='s', linewidth=2)\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Forecast Comparison (Sample)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. MULTI-STEP FORECASTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_future(model, last_sequence, steps, scaler, device):\n",
    "    \"\"\"Make multi-step future predictions\"\"\"\n",
    "    model.eval()\n",
    "    forecasts = []\n",
    "    current_seq = torch.FloatTensor(last_sequence).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps // CONFIG['forecast_horizon']):\n",
    "            pred = model(current_seq).cpu().numpy()\n",
    "            forecasts.append(pred[0])\n",
    "            \n",
    "            # Update sequence with prediction\n",
    "            pred_reshaped = pred.reshape(-1, 1)\n",
    "            current_seq = torch.FloatTensor(\n",
    "                np.vstack([current_seq.cpu().numpy()[0, CONFIG['forecast_horizon']:], pred_reshaped])\n",
    "            ).unsqueeze(0).to(device)\n",
    "    \n",
    "    forecasts = np.array(forecasts).flatten()\n",
    "    forecasts_actual = scaler.inverse_transform(forecasts.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return forecasts_actual\n",
    "\n",
    "# Forecast next 30 days\n",
    "last_sequence = scaled_data[-CONFIG['lookback_window']:]\n",
    "future_forecast = forecast_future(model, last_sequence, 30, scaler, device)\n",
    "\n",
    "# Visualize future forecast\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df_filled[CONFIG['target_column']][-100:], label='Historical', linewidth=2)\n",
    "future_dates = pd.date_range(df_filled.index[-1], periods=len(future_forecast)+1, freq='D')[1:]\n",
    "plt.plot(future_dates, future_forecast, label='Forecast', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Future Forecast')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. MODEL COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['LSTM', 'ARIMA', 'Exponential Smoothing'],\n",
    "    'RMSE': [metrics['RMSE'], 0.0, 0.0],  # Fill with actual values\n",
    "    'MAE': [metrics['MAE'], 0.0, 0.0],\n",
    "    'MAPE': [metrics['MAPE'], 0.0, 0.0]\n",
    "})\n",
    "\n",
    "comparison_df.plot(x='Model', y=['RMSE', 'MAE', 'MAPE'], kind='bar', figsize=(12, 6))\n",
    "plt.title('Model Comparison')\n",
    "plt.ylabel('Error')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. SAVE & DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'config': CONFIG\n",
    "}, 'timeseries_model.pth')\n",
    "\n",
    "# Save as ONNX\n",
    "# dummy_input = torch.randn(1, CONFIG['lookback_window'], 1).to(device)\n",
    "# torch.onnx.export(model, dummy_input, \"timeseries_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. CONCLUSIONS & NEXT STEPS\n",
    "\n",
    "## Summary:\n",
    "- Best Model: LSTM with X layers\n",
    "- RMSE: X.XX\n",
    "- Forecast Horizon: X days\n",
    "\n",
    "## Next Steps:\n",
    "- [ ] Implement attention mechanism\n",
    "- [ ] Try Prophet for seasonality\n",
    "- [ ] Add external features (weather, holidays)\n",
    "- [ ] Ensemble multiple models\n",
    "- [ ] Implement online learning\n",
    "- [ ] Deploy as API endpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
