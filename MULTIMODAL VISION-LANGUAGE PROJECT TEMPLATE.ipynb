{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIMODAL VISION-LANGUAGE PROJECT TEMPLATE\n",
    "===========================================\n",
    "Use Case: Image Captioning, VQA, Visual Document Understanding, OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PROJECT SETUP & ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch torchvision\n",
    "# !pip install pillow opencv-python\n",
    "# !pip install datasets evaluate\n",
    "# !pip install pytesseract easyocr\n",
    "# !pip install gradio streamlit\n",
    "# !pip install rouge-score nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HuggingFace\n",
    "import torch\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    BlipForQuestionAnswering,\n",
    "    AutoProcessor,\n",
    "    Pix2StructForConditionalGeneration,\n",
    "    TrOCRProcessor,\n",
    "    VisionEncoderDecoderModel as TrOCRModel\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Evaluation\n",
    "from evaluate import load as load_metric\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Task configuration\n",
    "    'task': 'image_captioning',  # 'image_captioning', 'vqa', 'ocr', 'document_understanding'\n",
    "    \n",
    "    # Model configuration\n",
    "    'model_name': 'Salesforce/blip-image-captioning-base',\n",
    "    # Alternatives:\n",
    "    # - 'nlpconnect/vit-gpt2-image-captioning'\n",
    "    # - 'Salesforce/blip-vqa-base'\n",
    "    # - 'microsoft/trocr-base-handwritten'\n",
    "    # - 'google/pix2struct-docvqa-base'\n",
    "    \n",
    "    # Data configuration\n",
    "    'data_dir': './data/images',\n",
    "    'batch_size': 8,\n",
    "    'num_workers': 4,\n",
    "    'max_length': 128,\n",
    "    \n",
    "    # Training configuration\n",
    "    'learning_rate': 5e-5,\n",
    "    'num_epochs': 5,\n",
    "    'warmup_steps': 500,\n",
    "    \n",
    "    # Generation configuration\n",
    "    'max_new_tokens': 50,\n",
    "    'num_beams': 4,\n",
    "    'temperature': 1.0,\n",
    "    \n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "np.random.seed(CONFIG['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA LOADING & EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_dataset(data_dir, dataset_type='custom'):\n",
    "    \"\"\"Load image dataset\"\"\"\n",
    "    \n",
    "    if dataset_type == 'huggingface':\n",
    "        # Load from HuggingFace\n",
    "        dataset = load_dataset('nlphuji/flickr30k')  # Example dataset\n",
    "        return dataset\n",
    "    \n",
    "    else:\n",
    "        # Load custom dataset\n",
    "        image_paths = list(Path(data_dir).glob('**/*.jpg')) + \\\n",
    "                     list(Path(data_dir).glob('**/*.png'))\n",
    "        \n",
    "        # Load annotations if available\n",
    "        annotations_file = Path(data_dir) / 'annotations.json'\n",
    "        if annotations_file.exists():\n",
    "            with open(annotations_file, 'r') as f:\n",
    "                annotations = json.load(f)\n",
    "        else:\n",
    "            annotations = {str(p): \"\" for p in image_paths}\n",
    "        \n",
    "        data = {\n",
    "            'image_path': [str(p) for p in image_paths],\n",
    "            'caption': [annotations.get(str(p), \"\") for p in image_paths]\n",
    "        }\n",
    "        \n",
    "        return Dataset.from_dict(data)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_image_dataset(CONFIG['data_dir'])\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=6):\n",
    "    \"\"\"Visualize sample images with captions\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        # Load image\n",
    "        if 'image' in sample:\n",
    "            image = sample['image']\n",
    "        else:\n",
    "            image = Image.open(sample['image_path'])\n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Show caption if available\n",
    "        if 'caption' in sample or 'text' in sample:\n",
    "            caption = sample.get('caption', sample.get('text', ''))\n",
    "            axes[i].set_title(caption[:50] + '...' if len(caption) > 50 else caption,\n",
    "                            fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Analyze Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image dimensions\n",
    "def analyze_images(dataset, sample_size=100):\n",
    "    \"\"\"Analyze image statistics\"\"\"\n",
    "    widths = []\n",
    "    heights = []\n",
    "    aspects = []\n",
    "    \n",
    "    sample_indices = np.random.choice(len(dataset), min(sample_size, len(dataset)), replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        sample = dataset[int(idx)]\n",
    "        \n",
    "        if 'image' in sample:\n",
    "            image = sample['image']\n",
    "        else:\n",
    "            image = Image.open(sample['image_path'])\n",
    "        \n",
    "        w, h = image.size\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "        aspects.append(w / h)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].hist(widths, bins=30, edgecolor='black')\n",
    "    axes[0].set_title('Width Distribution')\n",
    "    axes[0].set_xlabel('Width (pixels)')\n",
    "    \n",
    "    axes[1].hist(heights, bins=30, edgecolor='black')\n",
    "    axes[1].set_title('Height Distribution')\n",
    "    axes[1].set_xlabel('Height (pixels)')\n",
    "    \n",
    "    axes[2].hist(aspects, bins=30, edgecolor='black')\n",
    "    axes[2].set_title('Aspect Ratio Distribution')\n",
    "    axes[2].set_xlabel('Width / Height')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average width: {np.mean(widths):.0f}px\")\n",
    "    print(f\"Average height: {np.mean(heights):.0f}px\")\n",
    "    print(f\"Average aspect ratio: {np.mean(aspects):.2f}\")\n",
    "\n",
    "analyze_images(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. IMAGE CAPTIONING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Captioning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP model for image captioning\n",
    "processor = BlipProcessor.from_pretrained(CONFIG['model_name'])\n",
    "model = BlipForConditionalGeneration.from_pretrained(CONFIG['model_name']).to(device)\n",
    "\n",
    "print(f\"Model loaded: {CONFIG['model_name']}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Generate Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path, model, processor, device):\n",
    "    \"\"\"Generate caption for an image\"\"\"\n",
    "    # Load image\n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    else:\n",
    "        image = image_path\n",
    "    \n",
    "    # Process\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=CONFIG['max_new_tokens'],\n",
    "            num_beams=CONFIG['num_beams'],\n",
    "            temperature=CONFIG['temperature']\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Test Captioning on Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample images\n",
    "print(\"Generating captions for sample images...\\n\")\n",
    "\n",
    "num_samples = 4\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(min(num_samples, len(dataset))):\n",
    "    sample = dataset[i]\n",
    "    \n",
    "    # Load image\n",
    "    if 'image' in sample:\n",
    "        image = sample['image']\n",
    "    else:\n",
    "        image = Image.open(sample['image_path'])\n",
    "    \n",
    "    # Generate caption\n",
    "    caption = generate_caption(image, model, processor, device)\n",
    "    \n",
    "    # Display\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Generated: {caption}\", fontsize=10, wrap=True)\n",
    "    \n",
    "    print(f\"Image {i+1}:\")\n",
    "    print(f\"Generated Caption: {caption}\")\n",
    "    if 'caption' in sample:\n",
    "        print(f\"Ground Truth: {sample['caption']}\")\n",
    "    print()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. VISUAL QUESTION ANSWERING (VQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load VQA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP model for VQA\n",
    "vqa_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "vqa_model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
    "\n",
    "print(\"VQA model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Answer Questions about Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_visual_question(image_path, question, model, processor, device):\n",
    "    \"\"\"Answer a question about an image\"\"\"\n",
    "    # Load image\n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    else:\n",
    "        image = image_path\n",
    "    \n",
    "    # Process\n",
    "    inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=50)\n",
    "    \n",
    "    # Decode\n",
    "    answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Interactive VQA Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VQA on sample images\n",
    "sample_image = dataset[0]['image'] if 'image' in dataset[0] else Image.open(dataset[0]['image_path'])\n",
    "\n",
    "questions = [\n",
    "    \"What is in the image?\",\n",
    "    \"What color is the main object?\",\n",
    "    \"How many people are in the image?\",\n",
    "    \"What is the setting of this image?\"\n",
    "]\n",
    "\n",
    "print(\"Visual Question Answering Demo\\n\" + \"=\"*50)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sample_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Query Image\")\n",
    "plt.show()\n",
    "\n",
    "for question in questions:\n",
    "    answer = answer_visual_question(sample_image, question, vqa_model, vqa_processor, device)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. OPTICAL CHARACTER RECOGNITION (OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load TrOCR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TrOCR for handwritten text recognition\n",
    "trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").to(device)\n",
    "\n",
    "print(\"TrOCR model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Extract Text from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_trocr(image_path, model, processor, device):\n",
    "    \"\"\"Extract text from image using TrOCR\"\"\"\n",
    "    # Load image\n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    else:\n",
    "        image = image_path\n",
    "    \n",
    "    # Process\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values, max_new_tokens=CONFIG['max_length'])\n",
    "    \n",
    "    # Decode\n",
    "    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Alternative: EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "\n",
    "# Initialize EasyOCR\n",
    "reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "\n",
    "def extract_text_easyocr(image_path):\n",
    "    \"\"\"Extract text using EasyOCR\"\"\"\n",
    "    if isinstance(image_path, str):\n",
    "        image = cv2.imread(image_path)\n",
    "    else:\n",
    "        image = cv2.cvtColor(np.array(image_path), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Extract text\n",
    "    results = reader.readtext(image)\n",
    "    \n",
    "    # Combine text\n",
    "    text = \" \".join([result[1] for result in results])\n",
    "    \n",
    "    return text, results\n",
    "\n",
    "# Test OCR\n",
    "# text = extract_text_trocr(sample_image, trocr_model, trocr_processor, device)\n",
    "# print(f\"Extracted Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DOCUMENT UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Load Document VQA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pix2Struct for document understanding\n",
    "doc_processor = AutoProcessor.from_pretrained(\"google/pix2struct-docvqa-base\")\n",
    "doc_model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-docvqa-base\").to(device)\n",
    "\n",
    "print(\"Document understanding model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Answer Questions about Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_document_question(image_path, question, model, processor, device):\n",
    "    \"\"\"Answer questions about document images\"\"\"\n",
    "    # Load image\n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    else:\n",
    "        image = image_path\n",
    "    \n",
    "    # Process\n",
    "    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "    \n",
    "    # Decode\n",
    "    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test document understanding\n",
    "# document_image = Image.open(\"invoice.png\")\n",
    "# question = \"What is the total amount?\"\n",
    "# answer = answer_document_question(document_image, question, doc_model, doc_processor, device)\n",
    "# print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. BATCH PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Batch Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate_captions(image_paths, model, processor, device, batch_size=8):\n",
    "    \"\"\"Generate captions for multiple images\"\"\"\n",
    "    all_captions = []\n",
    "    \n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        \n",
    "        # Load images\n",
    "        images = []\n",
    "        for path in batch_paths:\n",
    "            if isinstance(path, str):\n",
    "                img = Image.open(path).convert('RGB')\n",
    "            else:\n",
    "                img = path\n",
    "            images.append(img)\n",
    "        \n",
    "        # Process batch\n",
    "        inputs = processor(images, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=CONFIG['max_new_tokens'])\n",
    "        \n",
    "        # Decode\n",
    "        captions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "        all_captions.extend(captions)\n",
    "    \n",
    "    return all_captions\n",
    "\n",
    "# Test batch processing\n",
    "# sample_images = [dataset[i]['image_path'] for i in range(min(10, len(dataset)))]\n",
    "# captions = batch_generate_captions(sample_images, model, processor, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    \"\"\"Calculate BLEU score\"\"\"\n",
    "    # Tokenize\n",
    "    refs = [[ref.split()] for ref in references]\n",
    "    hyps = [hyp.split() for hyp in hypotheses]\n",
    "    \n",
    "    # Calculate BLEU\n",
    "    bleu_1 = corpus_bleu(refs, hyps, weights=(1.0, 0, 0, 0))\n",
    "    bleu_2 = corpus_bleu(refs, hyps, weights=(0.5, 0.5, 0, 0))\n",
    "    bleu_3 = corpus_bleu(refs, hyps, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu_4 = corpus_bleu(refs, hyps, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    return {\n",
    "        'BLEU-1': bleu_1,\n",
    "        'BLEU-2': bleu_2,\n",
    "        'BLEU-3': bleu_3,\n",
    "        'BLEU-4': bleu_4\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(references, hypotheses):\n",
    "    \"\"\"Calculate ROUGE score\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        score = scorer.score(ref, hyp)\n",
    "        for key in scores:\n",
    "            scores[key].append(score[key].fmeasure)\n",
    "    \n",
    "    # Average scores\n",
    "    avg_scores = {key: np.mean(values) for key, values in scores.items()}\n",
    "    \n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 METEOR and CIDEr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "try:\n",
    "    meteor = load_metric('meteor')\n",
    "    # cider = load_metric('cider')\n",
    "    \n",
    "    def calculate_meteor(references, predictions):\n",
    "        \"\"\"Calculate METEOR score\"\"\"\n",
    "        score = meteor.compute(predictions=predictions, references=references)\n",
    "        return score['meteor']\n",
    "except:\n",
    "    print(\"Meteor metric not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_captioning_model(dataset, model, processor, device, num_samples=100):\n",
    "    \"\"\"Evaluate captioning model\"\"\"\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    sample_indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        sample = dataset[int(idx)]\n",
    "        \n",
    "        # Load image\n",
    "        if 'image' in sample:\n",
    "            image = sample['image']\n",
    "        else:\n",
    "            image = Image.open(sample['image_path'])\n",
    "        \n",
    "        # Generate caption\n",
    "        caption = generate_caption(image, model, processor, device)\n",
    "        \n",
    "        # Get reference\n",
    "        if 'caption' in sample:\n",
    "            reference = sample['caption']\n",
    "        elif 'text' in sample:\n",
    "            reference = sample['text']\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        references.append(reference)\n",
    "        hypotheses.append(caption)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu_scores = calculate_bleu(references, hypotheses)\n",
    "    rouge_scores = calculate_rouge(references, hypotheses)\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nBLEU Scores:\")\n",
    "    for key, value in bleu_scores.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    for key, value in rouge_scores.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    return bleu_scores, rouge_scores\n",
    "\n",
    "# Evaluate model\n",
    "# bleu_scores, rouge_scores = evaluate_captioning_model(dataset, model, processor, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. GRADIO INTERFACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Image Captioning Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def create_captioning_interface(model, processor, device):\n",
    "    \"\"\"Create Gradio interface for image captioning\"\"\"\n",
    "    \n",
    "    def caption_image(image):\n",
    "        caption = generate_caption(image, model, processor, device)\n",
    "        return caption\n",
    "    \n",
    "    interface = gr.Interface(\n",
    "        fn=caption_image,\n",
    "        inputs=gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
    "        outputs=gr.Textbox(label=\"Generated Caption\"),\n",
    "        title=\"Image Captioning\",\n",
    "        description=\"Upload an image to generate a caption\",\n",
    "        examples=[\n",
    "            # Add example image paths here\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Launch interface\n",
    "# interface = create_captioning_interface(model, processor, device)\n",
    "# interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 VQA Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vqa_interface(model, processor, device):\n",
    "    \"\"\"Create Gradio interface for VQA\"\"\"\n",
    "    \n",
    "    def answer_question(image, question):\n",
    "        answer = answer_visual_question(image, question, model, processor, device)\n",
    "        return answer\n",
    "    \n",
    "    interface = gr.Interface(\n",
    "        fn=answer_question,\n",
    "        inputs=[\n",
    "            gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
    "            gr.Textbox(label=\"Ask a Question\")\n",
    "        ],\n",
    "        outputs=gr.Textbox(label=\"Answer\"),\n",
    "        title=\"Visual Question Answering\",\n",
    "        description=\"Upload an image and ask questions about it\"\n",
    "    )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Launch VQA interface\n",
    "# vqa_interface = create_vqa_interface(vqa_model, vqa_processor, device)\n",
    "# vqa_interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Multi-Task Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multitask_interface():\n",
    "    \"\"\"Create interface with multiple vision tasks\"\"\"\n",
    "    \n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Multimodal Vision-Language System\")\n",
    "        \n",
    "        with gr.Tab(\"Image Captioning\"):\n",
    "            cap_image = gr.Image(type=\"pil\")\n",
    "            cap_button = gr.Button(\"Generate Caption\")\n",
    "            cap_output = gr.Textbox(label=\"Caption\")\n",
    "            \n",
    "            cap_button.click(\n",
    "                lambda img: generate_caption(img, model, processor, device),\n",
    "                inputs=cap_image,\n",
    "                outputs=cap_output\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"Visual QA\"):\n",
    "            vqa_image = gr.Image(type=\"pil\")\n",
    "            vqa_question = gr.Textbox(label=\"Question\")\n",
    "            vqa_button = gr.Button(\"Get Answer\")\n",
    "            vqa_output = gr.Textbox(label=\"Answer\")\n",
    "            \n",
    "            vqa_button.click(\n",
    "                lambda img, q: answer_visual_question(img, q, vqa_model, vqa_processor, device),\n",
    "                inputs=[vqa_image, vqa_question],\n",
    "                outputs=vqa_output\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"OCR\"):\n",
    "            ocr_image = gr.Image(type=\"pil\")\n",
    "            ocr_button = gr.Button(\"Extract Text\")\n",
    "            ocr_output = gr.Textbox(label=\"Extracted Text\")\n",
    "            \n",
    "            ocr_button.click(\n",
    "                lambda img: extract_text_trocr(img, trocr_model, trocr_processor, device),\n",
    "                inputs=ocr_image,\n",
    "                outputs=ocr_output\n",
    "            )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch multi-task interface\n",
    "# multi_interface = create_multitask_interface()\n",
    "# multi_interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. MODEL FINE-TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptioningDataset(Dataset):\n",
    "    \"\"\"Custom dataset for image captioning\"\"\"\n",
    "    \n",
    "    def __init__(self, data, processor, max_length=128):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        # Load image\n",
    "        if 'image' in sample:\n",
    "            image = sample['image']\n",
    "        else:\n",
    "            image = Image.open(sample['image_path']).convert('RGB')\n",
    "        \n",
    "        # Get caption\n",
    "        caption = sample.get('caption', sample.get('text', ''))\n",
    "        \n",
    "        # Process\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# Create dataset\n",
    "# train_dataset = CaptioningDataset(dataset, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def train_captioning_model(model, train_dataset, eval_dataset, output_dir='./finetuned_model'):\n",
    "    \"\"\"Fine-tune captioning model\"\"\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=CONFIG['num_epochs'],\n",
    "        per_device_train_batch_size=CONFIG['batch_size'],\n",
    "        per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "        warmup_steps=CONFIG['warmup_steps'],\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        logging_steps=100,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Fine-tune model\n",
    "# trainer = train_captioning_model(model, train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "def save_model(model, processor, output_dir='./saved_model'):\n",
    "    \"\"\"Save model and processor\"\"\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "# Load model\n",
    "def load_saved_model(model_dir):\n",
    "    \"\"\"Load saved model\"\"\"\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
    "    processor = BlipProcessor.from_pretrained(model_dir)\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. CONCLUSIONS & NEXT STEPS\n",
    "\n",
    "## Summary:\n",
    "- Task: {CONFIG['task']}\n",
    "- Model: {CONFIG['model_name']}\n",
    "- Generated captions for X images\n",
    "- Average BLEU score: X.XX\n",
    "\n",
    "## Next Steps:\n",
    "- [ ] Fine-tune on domain-specific data\n",
    "- [ ] Implement attention visualization\n",
    "- [ ] Add support for video captioning\n",
    "- [ ] Implement multi-language support\n",
    "- [ ] Optimize inference speed\n",
    "- [ ] Deploy as REST API\n",
    "- [ ] Add real-time webcam support\n",
    "- [ ] Implement image generation from text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
