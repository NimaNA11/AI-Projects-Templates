{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETRIEVAL-AUGMENTED GENERATION (RAG) SYSTEM TEMPLATE\n",
    "====================================================\n",
    "Use Case: Document Q&A, Knowledge Base, Chatbot with External Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PROJECT SETUP & ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-community langchain-openai\n",
    "# !pip install chromadb faiss-cpu sentence-transformers\n",
    "# !pip install pypdf python-docx unstructured\n",
    "# !pip install openai tiktoken\n",
    "# !pip install gradio streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain Core\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    DirectoryLoader,\n",
    "    TextLoader,\n",
    "    CSVLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# LLMs\n",
    "from langchain.llms import HuggingFacePipeline, OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Evaluation\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ['OPENAI_API_KEY'] = 'your-api-key-here'  # Or use local models\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Document settings\n",
    "    'data_dir': './documents',\n",
    "    'chunk_size': 1000,\n",
    "    'chunk_overlap': 200,\n",
    "    'separator': '\\n\\n',\n",
    "    \n",
    "    # Embedding settings\n",
    "    'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',  # or 'text-embedding-ada-002'\n",
    "    'embedding_device': 'cpu',\n",
    "    \n",
    "    # Vector store settings\n",
    "    'vector_store_type': 'faiss',  # 'faiss' or 'chroma'\n",
    "    'vector_store_path': './vector_store',\n",
    "    'collection_name': 'knowledge_base',\n",
    "    \n",
    "    # LLM settings\n",
    "    'llm_type': 'huggingface',  # 'openai' or 'huggingface'\n",
    "    'model_name': 'google/flan-t5-base',  # or 'gpt-3.5-turbo'\n",
    "    'temperature': 0.7,\n",
    "    'max_tokens': 512,\n",
    "    \n",
    "    # Retrieval settings\n",
    "    'top_k': 4,\n",
    "    'search_type': 'similarity',  # 'similarity' or 'mmr'\n",
    "    'mmr_diversity': 0.3,\n",
    "    \n",
    "    # Chain settings\n",
    "    'chain_type': 'stuff',  # 'stuff', 'map_reduce', 'refine', 'map_rerank'\n",
    "    'return_source_documents': True,\n",
    "    \n",
    "    'random_seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DOCUMENT LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Documents from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(data_dir, file_types=['pdf', 'txt', 'md', 'csv']):\n",
    "    \"\"\"Load documents from directory\"\"\"\n",
    "    documents = []\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    for file_type in file_types:\n",
    "        print(f\"Loading .{file_type} files...\")\n",
    "        \n",
    "        if file_type == 'pdf':\n",
    "            loader = DirectoryLoader(\n",
    "                data_dir,\n",
    "                glob=f\"**/*.{file_type}\",\n",
    "                loader_cls=PyPDFLoader,\n",
    "                show_progress=True\n",
    "            )\n",
    "        elif file_type == 'txt':\n",
    "            loader = DirectoryLoader(\n",
    "                data_dir,\n",
    "                glob=f\"**/*.{file_type}\",\n",
    "                loader_cls=TextLoader,\n",
    "                show_progress=True\n",
    "            )\n",
    "        elif file_type == 'md':\n",
    "            loader = DirectoryLoader(\n",
    "                data_dir,\n",
    "                glob=f\"**/*.{file_type}\",\n",
    "                loader_cls=UnstructuredMarkdownLoader,\n",
    "                show_progress=True\n",
    "            )\n",
    "        elif file_type == 'csv':\n",
    "            loader = DirectoryLoader(\n",
    "                data_dir,\n",
    "                glob=f\"**/*.{file_type}\",\n",
    "                loader_cls=CSVLoader,\n",
    "                show_progress=True\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "            print(f\"Loaded {len(docs)} documents from .{file_type} files\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_type} files: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents(CONFIG['data_dir'])\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Document Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document statistics\n",
    "if documents:\n",
    "    doc_lengths = [len(doc.page_content) for doc in documents]\n",
    "    \n",
    "    print(f\"\\nDocument Statistics:\")\n",
    "    print(f\"Total documents: {len(documents)}\")\n",
    "    print(f\"Average length: {np.mean(doc_lengths):.0f} characters\")\n",
    "    print(f\"Median length: {np.median(doc_lengths):.0f} characters\")\n",
    "    print(f\"Min length: {np.min(doc_lengths)} characters\")\n",
    "    print(f\"Max length: {np.max(doc_lengths)} characters\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(doc_lengths, bins=30, edgecolor='black')\n",
    "    plt.xlabel('Document Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Document Lengths')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(doc_lengths)\n",
    "    plt.ylabel('Document Length (characters)')\n",
    "    plt.title('Document Length Box Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nSample Document:\")\n",
    "    print(f\"Source: {documents[0].metadata}\")\n",
    "    print(f\"Content Preview: {documents[0].page_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TEXT SPLITTING & CHUNKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Configure Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose splitter type\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CONFIG['chunk_size'],\n",
    "    chunk_overlap=CONFIG['chunk_overlap'],\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Alternative: Token-based splitting\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "# \n",
    "# text_splitter = TokenTextSplitter(\n",
    "#     chunk_size=CONFIG['chunk_size'],\n",
    "#     chunk_overlap=CONFIG['chunk_overlap'],\n",
    "#     encoding_name=tokenizer.name_or_path\n",
    "# )\n",
    "\n",
    "print(f\"Text splitter configured:\")\n",
    "print(f\"Chunk size: {CONFIG['chunk_size']}\")\n",
    "print(f\"Chunk overlap: {CONFIG['chunk_overlap']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nTotal chunks created: {len(chunks)}\")\n",
    "print(f\"Average chunk size: {np.mean([len(chunk.page_content) for chunk in chunks]):.0f} characters\")\n",
    "\n",
    "# Visualize chunk distribution\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(chunk_lengths, bins=30, edgecolor='black')\n",
    "plt.xlabel('Chunk Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Chunk Lengths')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sources = [chunk.metadata.get('source', 'Unknown') for chunk in chunks]\n",
    "source_counts = pd.Series(sources).value_counts().head(10)\n",
    "source_counts.plot(kind='barh')\n",
    "plt.xlabel('Number of Chunks')\n",
    "plt.title('Top 10 Sources by Chunk Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample chunk\n",
    "print(f\"\\nSample Chunk:\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")\n",
    "print(f\"Content: {chunks[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. EMBEDDINGS GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: HuggingFace Embeddings (Local)\n",
    "if CONFIG['llm_type'] == 'huggingface':\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=CONFIG['embedding_model'],\n",
    "        model_kwargs={'device': CONFIG['embedding_device']},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "\n",
    "# Option 2: OpenAI Embeddings (API)\n",
    "else:\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        openai_api_key=os.environ.get('OPENAI_API_KEY')\n",
    "    )\n",
    "\n",
    "print(f\"Embedding model loaded: {CONFIG['embedding_model']}\")\n",
    "\n",
    "# Test embedding\n",
    "sample_text = \"This is a test sentence.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "print(f\"Embedding dimension: {len(sample_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Embedding Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_embeddings(embeddings, texts, num_samples=10):\n",
    "    \"\"\"Benchmark embedding generation speed\"\"\"\n",
    "    test_texts = texts[:num_samples]\n",
    "    \n",
    "    start = time.time()\n",
    "    _ = embeddings.embed_documents(test_texts)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Embedded {num_samples} texts in {elapsed:.2f}s\")\n",
    "    print(f\"Average: {elapsed/num_samples:.3f}s per text\")\n",
    "\n",
    "# Benchmark\n",
    "sample_texts = [chunk.page_content for chunk in chunks[:10]]\n",
    "benchmark_embeddings(embeddings, sample_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. VECTOR STORE CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating vector store...\")\n",
    "\n",
    "if CONFIG['vector_store_type'] == 'faiss':\n",
    "    # FAISS vector store (in-memory, fast)\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Save to disk\n",
    "    vectorstore.save_local(CONFIG['vector_store_path'])\n",
    "    print(f\"Vector store saved to {CONFIG['vector_store_path']}\")\n",
    "\n",
    "elif CONFIG['vector_store_type'] == 'chroma':\n",
    "    # Chroma vector store (persistent)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=CONFIG['collection_name'],\n",
    "        persist_directory=CONFIG['vector_store_path']\n",
    "    )\n",
    "    \n",
    "    vectorstore.persist()\n",
    "    print(f\"Vector store persisted to {CONFIG['vector_store_path']}\")\n",
    "\n",
    "print(f\"Vector store created with {vectorstore._collection.count() if hasattr(vectorstore, '_collection') else len(chunks)} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Load Existing Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_store(path, embeddings, store_type='faiss'):\n",
    "    \"\"\"Load existing vector store\"\"\"\n",
    "    if store_type == 'faiss':\n",
    "        vectorstore = FAISS.load_local(\n",
    "            path,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    elif store_type == 'chroma':\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=path,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=CONFIG['collection_name']\n",
    "        )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Load vector store\n",
    "# vectorstore = load_vector_store(CONFIG['vector_store_path'], embeddings, CONFIG['vector_store_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. RETRIEVAL TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_retrieval(query, vectorstore, k=4):\n",
    "    \"\"\"Test retrieval with a query\"\"\"\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Retrieve documents\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"\\n--- Result {i} ---\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Content: {doc.page_content[:300]}...\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain neural networks\",\n",
    "    \"How does training work?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    retrieved_docs = test_retrieval(query, vectorstore, k=CONFIG['top_k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Similarity Search with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_retrieval_with_scores(query, vectorstore, k=4):\n",
    "    \"\"\"Test retrieval with similarity scores\"\"\"\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Retrieve with scores\n",
    "    docs_with_scores = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(docs_with_scores, 1):\n",
    "        print(f\"\\n--- Result {i} (Score: {score:.4f}) ---\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    \n",
    "    return docs_with_scores\n",
    "\n",
    "# Test with scores\n",
    "query = \"What is deep learning?\"\n",
    "docs_with_scores = test_retrieval_with_scores(query, vectorstore, k=CONFIG['top_k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 MMR (Maximal Marginal Relevance) Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR search for diverse results\n",
    "def test_mmr_retrieval(query, vectorstore, k=4, fetch_k=20):\n",
    "    \"\"\"Test MMR retrieval for diverse results\"\"\"\n",
    "    print(f\"\\nQuery (MMR): {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    docs = vectorstore.max_marginal_relevance_search(\n",
    "        query,\n",
    "        k=k,\n",
    "        fetch_k=fetch_k,\n",
    "        lambda_mult=CONFIG['mmr_diversity']\n",
    "    )\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"\\n--- Result {i} ---\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Test MMR\n",
    "mmr_docs = test_mmr_retrieval(\"Explain AI\", vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. LLM INITIALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Initialize Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['llm_type'] == 'openai':\n",
    "    # OpenAI LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        temperature=CONFIG['temperature'],\n",
    "        max_tokens=CONFIG['max_tokens'],\n",
    "        openai_api_key=os.environ.get('OPENAI_API_KEY')\n",
    "    )\n",
    "\n",
    "elif CONFIG['llm_type'] == 'huggingface':\n",
    "    # HuggingFace Local LLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG['model_name'],\n",
    "        torch_dtype='auto',\n",
    "        device_map='auto'\n",
    "    )\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=CONFIG['max_tokens'],\n",
    "        temperature=CONFIG['temperature'],\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15\n",
    "    )\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(f\"LLM initialized: {CONFIG['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. RAG CHAIN CONSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Create Custom Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Basic RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=CONFIG['chain_type'],\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=CONFIG['search_type'],\n",
    "        search_kwargs={'k': CONFIG['top_k']}\n",
    "    ),\n",
    "    return_source_documents=CONFIG['return_source_documents'],\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"RetrievalQA chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory for conversation history\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key='answer'\n",
    ")\n",
    "\n",
    "# Conversational chain\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=CONFIG['search_type'],\n",
    "        search_kwargs={'k': CONFIG['top_k']}\n",
    "    ),\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Conversational Retrieval chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. QUERY & ANSWER GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Single Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, chain):\n",
    "    \"\"\"Ask a question and get an answer\"\"\"\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = chain({\"query\": question})\n",
    "    \n",
    "    answer = result['result']\n",
    "    source_docs = result.get('source_documents', [])\n",
    "    \n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    \n",
    "    if source_docs:\n",
    "        print(f\"\\nSources ({len(source_docs)}):\")\n",
    "        for i, doc in enumerate(source_docs, 1):\n",
    "            print(f\"\\n{i}. {doc.metadata.get('source', 'Unknown')}\")\n",
    "            print(f\"   {doc.page_content[:150]}...\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test questions\n",
    "questions = [\n",
    "    \"What is the main topic of the documents?\",\n",
    "    \"Explain the key concepts\",\n",
    "    \"What are the practical applications?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = ask_question(question, qa_chain)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Conversational Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, chain):\n",
    "    \"\"\"Have a conversation\"\"\"\n",
    "    result = chain({\"question\": message})\n",
    "    \n",
    "    answer = result['answer']\n",
    "    sources = result.get('source_documents', [])\n",
    "    \n",
    "    print(f\"User: {message}\")\n",
    "    print(f\"Assistant: {answer}\")\n",
    "    \n",
    "    if sources:\n",
    "        print(f\"\\nSources: {[s.metadata.get('source', 'Unknown') for s in sources]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Conversational example\n",
    "conversation = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you give me an example?\",\n",
    "    \"How is it different from traditional programming?\"\n",
    "]\n",
    "\n",
    "for msg in conversation:\n",
    "    chat(msg, conversational_chain)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. ADVANCED RAG TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Multi-Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Generate multiple query variations\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': CONFIG['top_k']}),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Test multi-query\n",
    "question = \"How does AI work?\"\n",
    "docs = multi_query_retriever.get_relevant_documents(question)\n",
    "print(f\"Retrieved {len(docs)} documents using multi-query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Compress retrieved documents\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever(search_kwargs={'k': CONFIG['top_k']})\n",
    ")\n",
    "\n",
    "# Test compressed retrieval\n",
    "compressed_docs = compression_retriever.get_relevant_documents(\"What is deep learning?\")\n",
    "print(f\"Compressed to {len(compressed_docs)} relevant documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Self-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "# Define metadata fields\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The source file of the document\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page number\",\n",
    "        type=\"integer\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Technical documentation about AI and machine learning\"\n",
    "\n",
    "# Create self-query retriever\n",
    "self_query_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectorstore,\n",
    "    document_contents=document_content_description,\n",
    "    metadata_field_info=metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Parent Document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Store for parent documents\n",
    "parent_store = InMemoryStore()\n",
    "\n",
    "# Create parent document retriever\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "parent_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=parent_store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "parent_retriever.add_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Create Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Q&A pairs for evaluation\n",
    "eval_data = {\n",
    "    'questions': [\n",
    "        \"What is machine learning?\",\n",
    "        \"Explain neural networks\",\n",
    "        \"How does training work?\"\n",
    "    ],\n",
    "    'ground_truth': [\n",
    "        \"Machine learning is a subset of AI...\",\n",
    "        \"Neural networks are computational models...\",\n",
    "        \"Training involves adjusting weights...\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate answers\n",
    "predictions = []\n",
    "for question in eval_data['questions']:\n",
    "    result = qa_chain({\"query\": question})\n",
    "    predictions.append(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Custom Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_relevance(question, answer, retrieved_docs, embeddings):\n",
    "    \"\"\"Evaluate answer relevance to retrieved context\"\"\"\n",
    "    # Embed question, answer, and context\n",
    "    question_emb = embeddings.embed_query(question)\n",
    "    answer_emb = embeddings.embed_query(answer)\n",
    "    context_embs = [embeddings.embed_query(doc.page_content) for doc in retrieved_docs]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    q_a_sim = cosine_similarity([question_emb], [answer_emb])[0][0]\n",
    "    a_c_sims = [cosine_similarity([answer_emb], [c_emb])[0][0] for c_emb in context_embs]\n",
    "    \n",
    "    avg_context_sim = np.mean(a_c_sims)\n",
    "    \n",
    "    print(f\"Question-Answer Similarity: {q_a_sim:.4f}\")\n",
    "    print(f\"Average Answer-Context Similarity: {avg_context_sim:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'question_answer_sim': q_a_sim,\n",
    "        'answer_context_sim': avg_context_sim\n",
    "    }\n",
    "\n",
    "# Evaluate a sample\n",
    "question = eval_data['questions'][0]\n",
    "result = qa_chain({\"query\": question})\n",
    "metrics = evaluate_relevance(\n",
    "    question,\n",
    "    result['result'],\n",
    "    result['source_documents'],\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. DEPLOYMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def create_gradio_interface(chain):\n",
    "    \"\"\"Create Gradio interface for RAG system\"\"\"\n",
    "    \n",
    "    def respond(message, chat_history):\n",
    "        result = chain({\"query\": message})\n",
    "        answer = result['result']\n",
    "        sources = result.get('source_documents', [])\n",
    "        \n",
    "        # Format source information\n",
    "        source_info = \"\\n\\nSources:\\n\"\n",
    "        for i, doc in enumerate(sources, 1):\n",
    "            source_info += f\"{i}. {doc.metadata.get('source', 'Unknown')}\\n\"\n",
    "        \n",
    "        response = answer + source_info\n",
    "        chat_history.append((message, response))\n",
    "        \n",
    "        return \"\", chat_history\n",
    "    \n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# RAG-based Q&A System\")\n",
    "        gr.Markdown(\"Ask questions about your documents!\")\n",
    "        \n",
    "        chatbot = gr.Chatbot()\n",
    "        msg = gr.Textbox(placeholder=\"Ask a question...\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "        \n",
    "        msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch Gradio interface\n",
    "# demo = create_gradio_interface(qa_chain)\n",
    "# demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Streamlit Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streamlit_app.py with this content:\n",
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load components\n",
    "@st.cache_resource\n",
    "def load_rag_system():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.load_local(\"./vector_store\", embeddings)\n",
    "    qa_chain = RetrievalQA.from_chain_type(...)\n",
    "    return qa_chain\n",
    "\n",
    "qa_chain = load_rag_system()\n",
    "\n",
    "st.title(\"ðŸ“š RAG Q&A System\")\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.header(\"Settings\")\n",
    "    k = st.slider(\"Number of documents to retrieve\", 1, 10, 4)\n",
    "\n",
    "# Chat interface\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "if prompt := st.chat_input(\"Ask a question\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    with st.chat_message(\"assistant\"):\n",
    "        result = qa_chain({\"query\": prompt})\n",
    "        response = result['result']\n",
    "        st.markdown(response)\n",
    "        \n",
    "        # Show sources\n",
    "        with st.expander(\"View Sources\"):\n",
    "            for i, doc in enumerate(result['source_documents'], 1):\n",
    "                st.write(f\"**Source {i}:** {doc.metadata.get('source')}\")\n",
    "                st.write(doc.page_content[:200] + \"...\")\n",
    "    \n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "# with open('streamlit_app.py', 'w') as f:\n",
    "#     f.write(streamlit_code)\n",
    "\n",
    "# Run with: streamlit run streamlit_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 FastAPI REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(title=\"RAG Q&A API\")\n",
    "\n",
    "# Load RAG system on startup\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    global qa_chain\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.load_local(\"./vector_store\", embeddings)\n",
    "    qa_chain = RetrievalQA.from_chain_type(...)\n",
    "\n",
    "class Question(BaseModel):\n",
    "    query: str\n",
    "    k: Optional[int] = 4\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: str\n",
    "    sources: List[dict]\n",
    "\n",
    "@app.post(\"/ask\", response_model=Answer)\n",
    "async def ask_question(question: Question):\n",
    "    try:\n",
    "        result = qa_chain({\"query\": question.query})\n",
    "        \n",
    "        sources = [\n",
    "            {\n",
    "                \"source\": doc.metadata.get(\"source\"),\n",
    "                \"content\": doc.page_content[:200]\n",
    "            }\n",
    "            for doc in result.get(\"source_documents\", [])\n",
    "        ]\n",
    "        \n",
    "        return Answer(answer=result[\"result\"], sources=sources)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "# with open('api.py', 'w') as f:\n",
    "#     f.write(fastapi_code)\n",
    "\n",
    "# Run with: uvicorn api:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. OPTIMIZATION & BEST PRACTICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Caching for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def cached_retrieval(query: str, k: int = 4):\n",
    "    \"\"\"Cache retrieval results for repeated queries\"\"\"\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 Hybrid Search (Keyword + Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# BM25 retriever (keyword-based)\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = CONFIG['top_k']\n",
    "\n",
    "# Semantic retriever\n",
    "semantic_retriever = vectorstore.as_retriever(search_kwargs={'k': CONFIG['top_k']})\n",
    "\n",
    "# Ensemble retriever (combines both)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, semantic_retriever],\n",
    "    weights=[0.5, 0.5]  # Equal weighting\n",
    ")\n",
    "\n",
    "# Test hybrid search\n",
    "hybrid_docs = ensemble_retriever.get_relevant_documents(\"What is AI?\")\n",
    "print(f\"Hybrid search retrieved {len(hybrid_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 Re-ranking Retrieved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Cross-encoder reranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "def rerank_documents(query, docs, top_k=4):\n",
    "    \"\"\"Rerank documents using cross-encoder\"\"\"\n",
    "    pairs = [[query, doc.page_content] for doc in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Sort by scores\n",
    "    ranked_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    reranked_docs = [docs[i] for i in ranked_indices]\n",
    "    \n",
    "    return reranked_docs\n",
    "\n",
    "# Test reranking\n",
    "query = \"Explain deep learning\"\n",
    "initial_docs = vectorstore.similarity_search(query, k=10)\n",
    "reranked_docs = rerank_documents(query, initial_docs, top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 Metadata Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by metadata\n",
    "def filtered_search(query, metadata_filter, k=4):\n",
    "    \"\"\"Search with metadata filtering\"\"\"\n",
    "    docs = vectorstore.similarity_search(\n",
    "        query,\n",
    "        k=k,\n",
    "        filter=metadata_filter\n",
    "    )\n",
    "    return docs\n",
    "\n",
    "# Example: Filter by source\n",
    "# docs = filtered_search(\"AI\", {\"source\": \"document1.pdf\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. MONITORING & LOGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 Query Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class QueryLogger:\n",
    "    \"\"\"Log queries and responses for analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, log_file='query_logs.jsonl'):\n",
    "        self.log_file = log_file\n",
    "    \n",
    "    def log_query(self, query, answer, sources, latency):\n",
    "        \"\"\"Log a query-answer pair\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'sources': [s.metadata.get('source') for s in sources],\n",
    "            'latency_ms': latency * 1000\n",
    "        }\n",
    "        \n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(json.dumps(log_entry) + '\\n')\n",
    "    \n",
    "    def analyze_logs(self):\n",
    "        \"\"\"Analyze query logs\"\"\"\n",
    "        logs = []\n",
    "        with open(self.log_file, 'r') as f:\n",
    "            for line in f:\n",
    "                logs.append(json.loads(line))\n",
    "        \n",
    "        df = pd.DataFrame(logs)\n",
    "        \n",
    "        print(\"\\nQuery Analytics:\")\n",
    "        print(f\"Total queries: {len(df)}\")\n",
    "        print(f\"Average latency: {df['latency_ms'].mean():.2f}ms\")\n",
    "        print(f\"\\nMost used sources:\")\n",
    "        all_sources = [s for sources in df['sources'] for s in sources]\n",
    "        print(pd.Series(all_sources).value_counts().head(5))\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize logger\n",
    "logger = QueryLogger()\n",
    "\n",
    "# Log queries\n",
    "import time\n",
    "\n",
    "def ask_and_log(question, chain, logger):\n",
    "    \"\"\"Ask question and log the interaction\"\"\"\n",
    "    start = time.time()\n",
    "    result = chain({\"query\": question})\n",
    "    latency = time.time() - start\n",
    "    \n",
    "    logger.log_query(\n",
    "        query=question,\n",
    "        answer=result['result'],\n",
    "        sources=result.get('source_documents', []),\n",
    "        latency=latency\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def monitor_system_resources():\n",
    "    \"\"\"Monitor system resource usage\"\"\"\n",
    "    # CPU\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    \n",
    "    # Memory\n",
    "    memory = psutil.virtual_memory()\n",
    "    memory_percent = memory.percent\n",
    "    \n",
    "    print(f\"\\nSystem Resources:\")\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")\n",
    "    print(f\"Memory Usage: {memory_percent}%\")\n",
    "\n",
    "# Monitor during query\n",
    "monitor_system_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. ADVANCED FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "def create_streaming_chain(llm, vectorstore):\n",
    "    \"\"\"Create chain with streaming responses\"\"\"\n",
    "    streaming_llm = llm\n",
    "    streaming_llm.streaming = True\n",
    "    streaming_llm.callbacks = [StreamingStdOutCallbackHandler()]\n",
    "    \n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=streaming_llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 Agent-based RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "# Create tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Knowledge Base\",\n",
    "        func=qa_chain.run,\n",
    "        description=\"Useful for answering questions about documents in the knowledge base\"\n",
    "    ),\n",
    "    # Add more tools as needed\n",
    "]\n",
    "\n",
    "# Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Use agent\n",
    "# response = agent.run(\"What is machine learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. TESTING & DEBUGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 Debug Retrieval Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_retrieval(query, vectorstore, k=10):\n",
    "    \"\"\"Debug retrieval to understand results\"\"\"\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Get more results than usual\n",
    "    docs_with_scores = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Analyze score distribution\n",
    "    scores = [score for _, score in docs_with_scores]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(scores)), scores)\n",
    "    plt.xlabel('Document Rank')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.title('Retrieval Scores Distribution')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop Retrieved Documents:\")\n",
    "    for i, (doc, score) in enumerate(docs_with_scores[:5], 1):\n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"   Source: {doc.metadata.get('source')}\")\n",
    "        print(f\"   Content: {doc.page_content[:150]}...\")\n",
    "\n",
    "# Debug a query\n",
    "debug_retrieval(\"What is neural networks?\", vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2 Test Chain Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chain_components(query, chain):\n",
    "    \"\"\"Test individual components of the chain\"\"\"\n",
    "    print(f\"Testing query: {query}\\n\")\n",
    "    \n",
    "    # Test retriever\n",
    "    print(\"1. Testing Retriever...\")\n",
    "    docs = chain.retriever.get_relevant_documents(query)\n",
    "    print(f\"   Retrieved {len(docs)} documents\")\n",
    "    \n",
    "    # Test LLM\n",
    "    print(\"\\n2. Testing LLM...\")\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = chain.combine_documents_chain.llm_chain.llm(prompt)\n",
    "    print(f\"   Generated response: {response[:100]}...\")\n",
    "    \n",
    "    # Test full chain\n",
    "    print(\"\\n3. Testing Full Chain...\")\n",
    "    result = chain({\"query\": query})\n",
    "    print(f\"   Final answer: {result['result'][:100]}...\")\n",
    "\n",
    "# Test components\n",
    "# test_chain_components(\"What is AI?\", qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. SAVE & LOAD CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rag_config(config, path='rag_config.json'):\n",
    "    \"\"\"Save RAG configuration\"\"\"\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"Configuration saved to {path}\")\n",
    "\n",
    "def load_rag_config(path='rag_config.json'):\n",
    "    \"\"\"Load RAG configuration\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "# Save configuration\n",
    "save_rag_config(CONFIG)\n",
    "\n",
    "# Load configuration\n",
    "# loaded_config = load_rag_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. CONCLUSIONS & NEXT STEPS\n",
    "\n",
    "## Summary:\n",
    "- Documents processed: {len(documents)}\n",
    "- Chunks created: {len(chunks)}\n",
    "- Vector store: {CONFIG['vector_store_type']}\n",
    "- LLM: {CONFIG['model_name']}\n",
    "- Retrieval method: {CONFIG['search_type']}\n",
    "\n",
    "## Next Steps:\n",
    "- [ ] Implement advanced retrieval strategies (hybrid search, reranking)\n",
    "- [ ] Add multi-modal support (images, tables)\n",
    "- [ ] Implement caching for improved performance\n",
    "- [ ] Set up monitoring and analytics\n",
    "- [ ] Deploy as production API\n",
    "- [ ] Add authentication and rate limiting\n",
    "- [ ] Implement feedback loop for continuous improvement\n",
    "- [ ] Create comprehensive test suite\n",
    "- [ ] Add support for multiple languages\n",
    "- [ ] Implement conversation summarization for long chats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
