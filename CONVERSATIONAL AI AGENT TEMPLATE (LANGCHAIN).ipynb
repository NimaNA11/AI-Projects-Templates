{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERSATIONAL AI AGENT TEMPLATE (LANGCHAIN)\n",
    "============================================\n",
    "Use Case: Task-Oriented Chatbot, Personal Assistant, Customer Support Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PROJECT SETUP & ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-community langchain-openai\n",
    "# !pip install openai anthropic\n",
    "# !pip install langchain-experimental\n",
    "# !pip install duckduckgo-search wikipedia\n",
    "# !pip install gradio streamlit\n",
    "# !pip install pandas matplotlib requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain Core\n",
    "from langchain.agents import (\n",
    "    Tool,\n",
    "    AgentExecutor,\n",
    "    create_react_agent,\n",
    "    create_structured_chat_agent,\n",
    "    initialize_agent,\n",
    "    AgentType\n",
    ")\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationEntityMemory\n",
    ")\n",
    "\n",
    "# LLMs\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.llms import OpenAI, HuggingFacePipeline\n",
    "\n",
    "# Tools\n",
    "from langchain.tools import (\n",
    "    DuckDuckGoSearchRun,\n",
    "    WikipediaQueryRun,\n",
    "    YouTubeSearchTool,\n",
    "    PythonREPLTool\n",
    ")\n",
    "from langchain.utilities import (\n",
    "    WikipediaAPIWrapper,\n",
    "    SerpAPIWrapper,\n",
    "    OpenWeatherMapAPIWrapper\n",
    ")\n",
    "\n",
    "# Chains\n",
    "from langchain.chains import (\n",
    "    LLMChain,\n",
    "    ConversationChain,\n",
    "    SimpleSequentialChain,\n",
    "    SequentialChain,\n",
    "    LLMMathChain\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API keys\n",
    "os.environ['OPENAI_API_KEY'] = 'your-api-key'\n",
    "os.environ['SERPAPI_API_KEY'] = 'your-serpapi-key'\n",
    "os.environ['OPENWEATHERMAP_API_KEY'] = 'your-weather-key'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # LLM configuration\n",
    "    'llm_provider': 'openai',  # 'openai', 'anthropic', 'huggingface'\n",
    "    'model_name': 'gpt-3.5-turbo',\n",
    "    'temperature': 0.7,\n",
    "    'max_tokens': 1000,\n",
    "    \n",
    "    # Agent configuration\n",
    "    'agent_type': 'structured-chat',  # 'zero-shot-react', 'structured-chat', 'conversational-react'\n",
    "    'max_iterations': 10,\n",
    "    'verbose': True,\n",
    "    \n",
    "    # Memory configuration\n",
    "    'memory_type': 'buffer',  # 'buffer', 'summary', 'window', 'entity'\n",
    "    'memory_window': 5,\n",
    "    \n",
    "    # Tool configuration\n",
    "    'enable_search': True,\n",
    "    'enable_wikipedia': True,\n",
    "    'enable_calculator': True,\n",
    "    'enable_python': False,  # Security risk if enabled\n",
    "    \n",
    "    'random_seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LLM INITIALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initialize Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['llm_provider'] == 'openai':\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        temperature=CONFIG['temperature'],\n",
    "        max_tokens=CONFIG['max_tokens'],\n",
    "        streaming=True,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "elif CONFIG['llm_provider'] == 'anthropic':\n",
    "    llm = ChatAnthropic(\n",
    "        model=CONFIG['model_name'],\n",
    "        temperature=CONFIG['temperature'],\n",
    "        max_tokens=CONFIG['max_tokens']\n",
    "    )\n",
    "else:\n",
    "    # HuggingFace local model\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=CONFIG['max_tokens']\n",
    "    )\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(f\"LLM initialized: {CONFIG['llm_provider']} - {CONFIG['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CUSTOM TOOLS CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['enable_search']:\n",
    "    search = DuckDuckGoSearchRun()\n",
    "    \n",
    "    search_tool = Tool(\n",
    "        name=\"Web Search\",\n",
    "        func=search.run,\n",
    "        description=\"Useful for searching the internet for current information. Input should be a search query.\"\n",
    "    )\n",
    "else:\n",
    "    search_tool = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Wikipedia Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['enable_wikipedia']:\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    \n",
    "    wiki_tool = Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"Useful for getting detailed information about topics from Wikipedia. Input should be a topic or entity name.\"\n",
    "    )\n",
    "else:\n",
    "    wiki_tool = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Calculator Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['enable_calculator']:\n",
    "    from langchain_experimental.llm_math.base import LLMMathChain\n",
    "    \n",
    "    math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "    \n",
    "    calculator_tool = Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=math_chain.run,\n",
    "        description=\"Useful for performing mathematical calculations. Input should be a mathematical expression.\"\n",
    "    )\n",
    "else:\n",
    "    calculator_tool = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Custom API Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get current weather for a location\"\"\"\n",
    "    try:\n",
    "        # Use OpenWeatherMap or similar API\n",
    "        api_key = os.environ.get('OPENWEATHERMAP_API_KEY', '')\n",
    "        if not api_key:\n",
    "            return \"Weather API key not configured\"\n",
    "        \n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={api_key}&units=metric\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            temp = data['main']['temp']\n",
    "            description = data['weather'][0]['description']\n",
    "            return f\"Weather in {location}: {temp}Â°C, {description}\"\n",
    "        else:\n",
    "            return f\"Could not fetch weather for {location}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching weather: {str(e)}\"\n",
    "\n",
    "weather_tool = Tool(\n",
    "    name=\"Weather\",\n",
    "    func=get_weather,\n",
    "    description=\"Get current weather information for a location. Input should be a city name.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_time() -> str:\n",
    "    \"\"\"Get current date and time\"\"\"\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "time_tool = Tool(\n",
    "    name=\"Current Time\",\n",
    "    func=lambda x: get_current_time(),\n",
    "    description=\"Get the current date and time. No input needed.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reminder(reminder_text: str) -> str:\n",
    "    \"\"\"Create a reminder (simulated)\"\"\"\n",
    "    # In production, this would save to a database\n",
    "    with open('reminders.txt', 'a') as f:\n",
    "        f.write(f\"{datetime.now()}: {reminder_text}\\n\")\n",
    "    return f\"Reminder created: {reminder_text}\"\n",
    "\n",
    "reminder_tool = Tool(\n",
    "    name=\"Create Reminder\",\n",
    "    func=create_reminder,\n",
    "    description=\"Create a reminder for later. Input should be the reminder text.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Database Query Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(query: str) -> str:\n",
    "    \"\"\"Query a database (simulated)\"\"\"\n",
    "    # Simulated database\n",
    "    database = {\n",
    "        \"user_count\": 1000,\n",
    "        \"active_users\": 750,\n",
    "        \"revenue\": \"$50,000\"\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    if \"user\" in query_lower and \"count\" in query_lower:\n",
    "        return f\"Total users: {database['user_count']}\"\n",
    "    elif \"active\" in query_lower:\n",
    "        return f\"Active users: {database['active_users']}\"\n",
    "    elif \"revenue\" in query_lower:\n",
    "        return f\"Revenue: {database['revenue']}\"\n",
    "    else:\n",
    "        return \"Available metrics: user_count, active_users, revenue\"\n",
    "\n",
    "database_tool = Tool(\n",
    "    name=\"Database Query\",\n",
    "    func=query_database,\n",
    "    description=\"Query the database for metrics and statistics. Input should be a query about users, revenue, or activity.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Assemble Tools List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "\n",
    "if search_tool:\n",
    "    tools.append(search_tool)\n",
    "if wiki_tool:\n",
    "    tools.append(wiki_tool)\n",
    "if calculator_tool:\n",
    "    tools.append(calculator_tool)\n",
    "\n",
    "tools.extend([weather_tool, time_tool, reminder_tool, database_tool])\n",
    "\n",
    "print(f\"\\nAvailable tools ({len(tools)}):\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MEMORY CONFIGURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Initialize Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['memory_type'] == 'buffer':\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "elif CONFIG['memory_type'] == 'summary':\n",
    "    memory = ConversationSummaryMemory(\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "elif CONFIG['memory_type'] == 'window':\n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        k=CONFIG['memory_window'],\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "elif CONFIG['memory_type'] == 'entity':\n",
    "    memory = ConversationEntityMemory(\n",
    "        llm=llm,\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "print(f\"Memory initialized: {CONFIG['memory_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. AGENT CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful AI assistant with access to various tools.\n",
    "\n",
    "Your goal is to help users with their requests by:\n",
    "1. Understanding what they need\n",
    "2. Using the appropriate tools when necessary\n",
    "3. Providing clear and helpful responses\n",
    "4. Asking for clarification if needed\n",
    "\n",
    "When using tools:\n",
    "- Choose the most appropriate tool for the task\n",
    "- Provide clear inputs to tools\n",
    "- Explain your reasoning\n",
    "\n",
    "Always be polite, professional, and helpful.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=CONFIG['verbose'],\n",
    "    max_iterations=CONFIG['max_iterations'],\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Update system message\n",
    "agent.agent.llm_chain.prompt.messages[0] = SystemMessagePromptTemplate.from_template(\n",
    "    system_prompt\n",
    ")\n",
    "\n",
    "print(\"Agent initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CONVERSATION INTERFACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Basic Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, agent, memory):\n",
    "    \"\"\"Send a message to the agent\"\"\"\n",
    "    try:\n",
    "        response = agent.run(message)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Interactive Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat(agent):\n",
    "    \"\"\"Interactive chat loop\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AI Assistant - Type 'quit' to exit\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"Assistant: Goodbye! Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        if not user_input.strip():\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nAssistant: \", end=\"\")\n",
    "        response = chat(user_input, agent, memory)\n",
    "        print(response + \"\\n\")\n",
    "\n",
    "# Run interactive chat\n",
    "# interactive_chat(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Test Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conversations = [\n",
    "    \"What's the weather like in London?\",\n",
    "    \"Can you search for the latest news about artificial intelligence?\",\n",
    "    \"What is 25 * 17 + 103?\",\n",
    "    \"Tell me about Albert Einstein\",\n",
    "    \"What time is it now?\",\n",
    "    \"Create a reminder to buy groceries tomorrow\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST CONVERSATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for message in test_conversations:\n",
    "    print(f\"\\nUser: {message}\")\n",
    "    print(\"Assistant: \", end=\"\")\n",
    "    response = chat(message, agent, memory)\n",
    "    print(response)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. MULTI-TURN CONVERSATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Context-Aware Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_turn_conversation = [\n",
    "    \"I'm planning a trip to Paris\",\n",
    "    \"What's the weather there?\",\n",
    "    \"Can you search for popular tourist attractions?\",\n",
    "    \"How about the Louvre Museum?\",\n",
    "    \"Create a reminder to book tickets\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-TURN CONVERSATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reset memory for clean start\n",
    "memory.clear()\n",
    "\n",
    "for message in multi_turn_conversation:\n",
    "    print(f\"\\nUser: {message}\")\n",
    "    print(\"Assistant: \", end=\"\")\n",
    "    response = chat(message, agent, memory)\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 View Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONVERSATION HISTORY\")\n",
    "print(\"=\"*60)\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. SPECIALIZED AGENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Customer Support Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_support_prompt = \"\"\"You are a customer support AI assistant for an e-commerce company.\n",
    "\n",
    "Your responsibilities:\n",
    "- Help customers with order inquiries\n",
    "- Provide product information\n",
    "- Handle returns and refunds\n",
    "- Address complaints professionally\n",
    "- Escalate complex issues to human agents\n",
    "\n",
    "Always:\n",
    "- Be empathetic and understanding\n",
    "- Provide clear solutions\n",
    "- Ask for order numbers when relevant\n",
    "- Follow company policies\n",
    "\n",
    "Available actions:\n",
    "- Check order status\n",
    "- Process returns\n",
    "- Look up product details\n",
    "- Create support tickets\"\"\"\n",
    "\n",
    "def create_customer_support_agent(llm, tools):\n",
    "    \"\"\"Create specialized customer support agent\"\"\"\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True\n",
    "    )\n",
    "    \n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        memory=memory,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Personal Assistant Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_assistant_prompt = \"\"\"You are a personal AI assistant helping with daily tasks.\n",
    "\n",
    "Capabilities:\n",
    "- Schedule management\n",
    "- Reminders and todos\n",
    "- Information lookup\n",
    "- Email and message drafting\n",
    "- Travel planning\n",
    "- General questions\n",
    "\n",
    "Personality:\n",
    "- Friendly and casual\n",
    "- Proactive in suggestions\n",
    "- Organized and efficient\n",
    "- Respectful of privacy\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Data Analyst Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_analyst_agent(llm):\n",
    "    \"\"\"Create agent specialized in data analysis\"\"\"\n",
    "    \n",
    "    # Add data analysis tools\n",
    "    def analyze_data(query: str) -> str:\n",
    "        \"\"\"Analyze data based on query\"\"\"\n",
    "        # Simulated analysis\n",
    "        return f\"Analysis result for: {query}\"\n",
    "    \n",
    "    data_tool = Tool(\n",
    "        name=\"Data Analysis\",\n",
    "        func=analyze_data,\n",
    "        description=\"Analyze data and generate insights\"\n",
    "    )\n",
    "    \n",
    "    analysis_tools = [data_tool, calculator_tool, database_tool]\n",
    "    \n",
    "    agent = initialize_agent(\n",
    "        tools=analysis_tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. ADVANCED FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Tool Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "def smart_tool_selection(query: str, tools: list) -> list:\n",
    "    \"\"\"Select most relevant tools for a query\"\"\"\n",
    "    # Implement embedding-based tool selection\n",
    "    relevant_tools = []\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Simple keyword matching (can be improved with embeddings)\n",
    "    if any(word in query_lower for word in ['weather', 'temperature', 'climate']):\n",
    "        relevant_tools.append([t for t in tools if t.name == 'Weather'][0])\n",
    "    \n",
    "    if any(word in query_lower for word in ['search', 'find', 'look up']):\n",
    "        relevant_tools.append([t for t in tools if t.name == 'Web Search'][0])\n",
    "    \n",
    "    if any(word in query_lower for word in ['calculate', 'math', 'compute']):\n",
    "        relevant_tools.append([t for t in tools if t.name == 'Calculator'][0])\n",
    "    \n",
    "    # Return all tools if no specific match\n",
    "    return relevant_tools if relevant_tools else tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "intent_template = \"\"\"Classify the user's intent into one of these categories:\n",
    "- question: User is asking for information\n",
    "- task: User wants to perform an action\n",
    "- conversation: Casual conversation\n",
    "- complaint: User has an issue\n",
    "\n",
    "User message: {message}\n",
    "\n",
    "Intent:\"\"\"\n",
    "\n",
    "intent_prompt = PromptTemplate(template=intent_template, input_variables=[\"message\"])\n",
    "intent_chain = LLMChain(llm=llm, prompt=intent_prompt)\n",
    "\n",
    "def classify_intent(message: str) -> str:\n",
    "    \"\"\"Classify user intent\"\"\"\n",
    "    intent = intent_chain.run(message=message).strip().lower()\n",
    "    return intent\n",
    "\n",
    "# Test intent classification\n",
    "test_messages = [\n",
    "    \"What's the capital of France?\",\n",
    "    \"Create a reminder for my meeting\",\n",
    "    \"How are you doing today?\",\n",
    "    \"This product is broken!\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTENT CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for msg in test_messages:\n",
    "    intent = classify_intent(msg)\n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Intent: {intent}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_template = \"\"\"Analyze the sentiment of this message:\n",
    "\"{message}\"\n",
    "\n",
    "Sentiment (positive/negative/neutral):\"\"\"\n",
    "\n",
    "sentiment_prompt = PromptTemplate(template=sentiment_template, input_variables=[\"message\"])\n",
    "sentiment_chain = LLMChain(llm=llm, prompt=sentiment_prompt)\n",
    "\n",
    "def analyze_sentiment(message: str) -> str:\n",
    "    \"\"\"Analyze message sentiment\"\"\"\n",
    "    sentiment = sentiment_chain.run(message=message).strip().lower()\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Response Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(user_message: str, agent_response: str) -> dict:\n",
    "    \"\"\"Evaluate the quality of agent response\"\"\"\n",
    "    metrics = {\n",
    "        'length': len(agent_response),\n",
    "        'has_greeting': any(word in agent_response.lower() for word in ['hello', 'hi', 'hey']),\n",
    "        'is_polite': any(word in agent_response.lower() for word in ['please', 'thank', 'sorry']),\n",
    "        'sentiment': analyze_sentiment(agent_response)\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. GRADIO INTERFACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Create Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def create_chatbot_interface(agent):\n",
    "    \"\"\"Create Gradio chatbot interface\"\"\"\n",
    "    \n",
    "    def respond(message, chat_history):\n",
    "        # Get response from agent\n",
    "        bot_response = chat(message, agent, memory)\n",
    "        \n",
    "        # Update history\n",
    "        chat_history.append((message, bot_response))\n",
    "        \n",
    "        return \"\", chat_history\n",
    "    \n",
    "    with gr.Blocks(title=\"AI Assistant\") as demo:\n",
    "        gr.Markdown(\"# ðŸ¤– AI Assistant\")\n",
    "        gr.Markdown(\"Ask me anything! I can search the web, do calculations, and more.\")\n",
    "        \n",
    "        chatbot = gr.Chatbot(height=500)\n",
    "        msg = gr.Textbox(\n",
    "            placeholder=\"Type your message here...\",\n",
    "            label=\"Message\"\n",
    "        )\n",
    "        \n",
    "        with gr.Row():\n",
    "            submit = gr.Button(\"Send\")\n",
    "            clear = gr.Button(\"Clear\")\n",
    "        \n",
    "        # Examples\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                \"What's the weather in Tokyo?\",\n",
    "                \"Calculate 15% of 250\",\n",
    "                \"Search for latest AI news\",\n",
    "                \"What time is it?\"\n",
    "            ],\n",
    "            inputs=msg\n",
    "        )\n",
    "        \n",
    "        msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "        submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Launch chatbot\n",
    "# demo = create_chatbot_interface(agent)\n",
    "# demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Multi-Agent Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_agent_interface():\n",
    "    \"\"\"Create interface with multiple specialized agents\"\"\"\n",
    "    \n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Multi-Agent System\")\n",
    "        \n",
    "        with gr.Tab(\"General Assistant\"):\n",
    "            chat1 = gr.Chatbot()\n",
    "            msg1 = gr.Textbox()\n",
    "            send1 = gr.Button(\"Send\")\n",
    "        \n",
    "        with gr.Tab(\"Customer Support\"):\n",
    "            chat2 = gr.Chatbot()\n",
    "            msg2 = gr.Textbox()\n",
    "            send2 = gr.Button(\"Send\")\n",
    "        \n",
    "        with gr.Tab(\"Data Analyst\"):\n",
    "            chat3 = gr.Chatbot()\n",
    "            msg3 = gr.Textbox()\n",
    "            send3 = gr.Button(\"Send\")\n",
    "    \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. LOGGING & MONITORING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Conversation Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ConversationLogger:\n",
    "    \"\"\"Log conversations for analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, log_file='conversations.jsonl'):\n",
    "        self.log_file = log_file\n",
    "    \n",
    "    def log_interaction(self, user_message, agent_response, metadata=None):\n",
    "        \"\"\"Log a single interaction\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'user_message': user_message,\n",
    "            'agent_response': agent_response,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(json.dumps(log_entry) + '\\n')\n",
    "    \n",
    "    def analyze_logs(self):\n",
    "        \"\"\"Analyze conversation logs\"\"\"\n",
    "        logs = []\n",
    "        with open(self.log_file, 'r') as f:\n",
    "            for line in f:\n",
    "                logs.append(json.loads(line))\n",
    "        \n",
    "        df = pd.DataFrame(logs)\n",
    "        \n",
    "        print(\"\\nConversation Analytics:\")\n",
    "        print(f\"Total interactions: {len(df)}\")\n",
    "        print(f\"Average response length: {df['agent_response'].str.len().mean():.0f} chars\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "logger = ConversationLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_response_time(func, *args):\n",
    "    \"\"\"Measure function execution time\"\"\"\n",
    "    start = time.time()\n",
    "    result = func(*args)\n",
    "    elapsed = time.time() - start\n",
    "    return result, elapsed\n",
    "\n",
    "# Test response time\n",
    "message = \"What's 25 * 17?\"\n",
    "response, latency = measure_response_time(chat, message, agent, memory)\n",
    "print(f\"Response time: {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. DEPLOYMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 FastAPI REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(title=\"AI Assistant API\")\n",
    "\n",
    "class Message(BaseModel):\n",
    "    text: str\n",
    "    user_id: str = \"default\"\n",
    "\n",
    "class Response(BaseModel):\n",
    "    response: str\n",
    "    timestamp: str\n",
    "\n",
    "@app.post(\"/chat\", response_model=Response)\n",
    "async def chat_endpoint(message: Message):\n",
    "    try:\n",
    "        response = agent.run(message.text)\n",
    "        return Response(\n",
    "            response=response,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. CONCLUSIONS & NEXT STEPS\n",
    "\n",
    "## Summary:\n",
    "- Agent Type: {CONFIG['agent_type']}\n",
    "- Tools: {len(tools)}\n",
    "- Memory: {CONFIG['memory_type']}\n",
    "- Average Response Time: X.XXs\n",
    "\n",
    "## Next Steps:\n",
    "- [ ] Add more specialized tools\n",
    "- [ ] Implement multi-agent orchestration\n",
    "- [ ] Add voice input/output\n",
    "- [ ] Implement user authentication\n",
    "- [ ] Add conversation analytics dashboard\n",
    "- [ ] Implement feedback collection\n",
    "- [ ] Add safety filters and content moderation\n",
    "- [ ] Deploy to production with scaling\n",
    "- [ ] Implement A/B testing for prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
