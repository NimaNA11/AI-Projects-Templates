{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECOMMENDATION SYSTEM PROJECT TEMPLATE\n",
    "======================================\n",
    "Use Case: Movie Recommendations, Product Recommendations, Content Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PROJECT SETUP & ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import svds\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_path': 'ratings.csv',\n",
    "    'items_path': 'items.csv',\n",
    "    'test_size': 0.2,\n",
    "    'min_ratings': 5,  # Minimum ratings per user/item\n",
    "    'latent_factors': 50,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 20,\n",
    "    'top_k': 10,  # Number of recommendations\n",
    "    'embedding_dim': 32,\n",
    "    'random_seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA LOADING & EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ratings data (user_id, item_id, rating, timestamp)\n",
    "ratings_df = pd.read_csv(CONFIG['data_path'])\n",
    "\n",
    "# Load item metadata (item_id, title, genre, etc.)\n",
    "items_df = pd.read_csv(CONFIG['items_path'])\n",
    "\n",
    "print(f\"Ratings shape: {ratings_df.shape}\")\n",
    "print(f\"Items shape: {items_df.shape}\")\n",
    "\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"\\n=== RATINGS STATISTICS ===\")\n",
    "print(f\"Number of users: {ratings_df['user_id'].nunique()}\")\n",
    "print(f\"Number of items: {ratings_df['item_id'].nunique()}\")\n",
    "print(f\"Number of ratings: {len(ratings_df)}\")\n",
    "print(f\"Rating range: {ratings_df['rating'].min()} - {ratings_df['rating'].max()}\")\n",
    "print(f\"Average rating: {ratings_df['rating'].mean():.2f}\")\n",
    "\n",
    "# Sparsity\n",
    "n_users = ratings_df['user_id'].nunique()\n",
    "n_items = ratings_df['item_id'].nunique()\n",
    "sparsity = 1 - (len(ratings_df) / (n_users * n_items))\n",
    "print(f\"Data sparsity: {sparsity:.4f} ({sparsity*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Rating value distribution\n",
    "axes[0].hist(ratings_df['rating'], bins=20, edgecolor='black')\n",
    "axes[0].set_title('Rating Distribution')\n",
    "axes[0].set_xlabel('Rating')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Ratings per user\n",
    "user_counts = ratings_df.groupby('user_id').size()\n",
    "axes[1].hist(user_counts, bins=50, edgecolor='black')\n",
    "axes[1].set_title('Ratings per User')\n",
    "axes[1].set_xlabel('Number of Ratings')\n",
    "axes[1].set_ylabel('Number of Users')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "# Ratings per item\n",
    "item_counts = ratings_df.groupby('item_id').size()\n",
    "axes[2].hist(item_counts, bins=50, edgecolor='black')\n",
    "axes[2].set_title('Ratings per Item')\n",
    "axes[2].set_xlabel('Number of Ratings')\n",
    "axes[2].set_ylabel('Number of Items')\n",
    "axes[2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top rated items\n",
    "top_items = ratings_df.groupby('item_id').agg({\n",
    "    'rating': ['count', 'mean']\n",
    "}).reset_index()\n",
    "top_items.columns = ['item_id', 'count', 'avg_rating']\n",
    "top_items = top_items[top_items['count'] >= 50].sort_values('avg_rating', ascending=False).head(10)\n",
    "\n",
    "# Merge with item metadata\n",
    "top_items = top_items.merge(items_df, on='item_id')\n",
    "print(\"\\nTop 10 Rated Items:\")\n",
    "print(top_items[['title', 'avg_rating', 'count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Filter Low Activity Users/Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count ratings per user and item\n",
    "user_activity = ratings_df.groupby('user_id').size()\n",
    "item_activity = ratings_df.groupby('item_id').size()\n",
    "\n",
    "# Filter\n",
    "active_users = user_activity[user_activity >= CONFIG['min_ratings']].index\n",
    "active_items = item_activity[item_activity >= CONFIG['min_ratings']].index\n",
    "\n",
    "ratings_filtered = ratings_df[\n",
    "    (ratings_df['user_id'].isin(active_users)) & \n",
    "    (ratings_df['item_id'].isin(active_items))\n",
    "]\n",
    "\n",
    "print(f\"Original ratings: {len(ratings_df)}\")\n",
    "print(f\"Filtered ratings: {len(ratings_filtered)}\")\n",
    "print(f\"Reduction: {(1 - len(ratings_filtered)/len(ratings_df))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping dictionaries\n",
    "user_ids = ratings_filtered['user_id'].unique()\n",
    "item_ids = ratings_filtered['item_id'].unique()\n",
    "\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "item_to_idx = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "\n",
    "idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}\n",
    "idx_to_item = {idx: item_id for item_id, idx in item_to_idx.items()}\n",
    "\n",
    "# Map to indices\n",
    "ratings_filtered['user_idx'] = ratings_filtered['user_id'].map(user_to_idx)\n",
    "ratings_filtered['item_idx'] = ratings_filtered['item_id'].map(item_to_idx)\n",
    "\n",
    "# Create sparse matrix\n",
    "n_users = len(user_ids)\n",
    "n_items = len(item_ids)\n",
    "\n",
    "user_item_matrix = sparse.csr_matrix(\n",
    "    (ratings_filtered['rating'].values,\n",
    "     (ratings_filtered['user_idx'].values, ratings_filtered['item_idx'].values)),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "print(f\"User-Item Matrix shape: {user_item_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    ratings_filtered, \n",
    "    test_size=CONFIG['test_size'], \n",
    "    random_state=CONFIG['random_seed']\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. COLLABORATIVE FILTERING - MATRIX FACTORIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 SVD-based Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_svd(user_item_matrix, k=50):\n",
    "    \"\"\"Perform SVD on user-item matrix\"\"\"\n",
    "    # Subtract mean rating\n",
    "    mean_rating = user_item_matrix.data.mean()\n",
    "    user_item_normalized = user_item_matrix.copy()\n",
    "    user_item_normalized.data -= mean_rating\n",
    "    \n",
    "    # Perform SVD\n",
    "    U, sigma, Vt = svds(user_item_normalized, k=k)\n",
    "    \n",
    "    # Reconstruct matrix\n",
    "    sigma = np.diag(sigma)\n",
    "    predicted_ratings = np.dot(np.dot(U, sigma), Vt) + mean_rating\n",
    "    \n",
    "    return predicted_ratings, U, sigma, Vt, mean_rating\n",
    "\n",
    "# Perform SVD\n",
    "predicted_ratings, U, sigma, Vt, mean_rating = matrix_factorization_svd(\n",
    "    user_item_matrix, \n",
    "    k=CONFIG['latent_factors']\n",
    ")\n",
    "\n",
    "print(f\"User factors shape: {U.shape}\")\n",
    "print(f\"Item factors shape: {Vt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCFDataset(Dataset):\n",
    "    \"\"\"Dataset for Neural Collaborative Filtering\"\"\"\n",
    "    def __init__(self, ratings_df):\n",
    "        self.users = ratings_df['user_idx'].values\n",
    "        self.items = ratings_df['item_idx'].values\n",
    "        self.ratings = ratings_df['rating'].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.LongTensor([self.users[idx]]),\n",
    "            torch.LongTensor([self.items[idx]]),\n",
    "            torch.FloatTensor([self.ratings[idx]])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCF(nn.Module):\n",
    "    \"\"\"Neural Collaborative Filtering Model\"\"\"\n",
    "    def __init__(self, num_users, num_items, embedding_dim=32, layers=[64, 32, 16, 8]):\n",
    "        super(NeuralCF, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        input_dim = embedding_dim * 2\n",
    "        \n",
    "        for layer_size in layers:\n",
    "            self.fc_layers.append(nn.Linear(input_dim, layer_size))\n",
    "            input_dim = layer_size\n",
    "        \n",
    "        self.output_layer = nn.Linear(layers[-1], 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "    \n",
    "    def forward(self, user_idx, item_idx):\n",
    "        # Get embeddings\n",
    "        user_emb = self.user_embedding(user_idx).squeeze(1)\n",
    "        item_emb = self.item_embedding(item_idx).squeeze(1)\n",
    "        \n",
    "        # Concatenate\n",
    "        x = torch.cat([user_emb, item_emb], dim=1)\n",
    "        \n",
    "        # MLP layers with ReLU\n",
    "        for fc in self.fc_layers:\n",
    "            x = F.relu(fc(x))\n",
    "        \n",
    "        # Output\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralCF(\n",
    "    num_users=n_users,\n",
    "    num_items=n_items,\n",
    "    embedding_dim=CONFIG['embedding_dim']\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataset = NCFDataset(train_data)\n",
    "test_dataset = NCFDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for user, item, rating in dataloader:\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "        rating = rating.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model(user, item)\n",
    "        loss = criterion(predictions, rating)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions_list = []\n",
    "    actuals_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user, item, rating in dataloader:\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            rating = rating.to(device)\n",
    "            \n",
    "            predictions = model(user, item)\n",
    "            loss = criterion(predictions, rating)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions_list.extend(predictions.cpu().numpy().flatten())\n",
    "            actuals_list.extend(rating.cpu().numpy().flatten())\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actuals_list, predictions_list))\n",
    "    mae = mean_absolute_error(actuals_list, predictions_list)\n",
    "    \n",
    "    return total_loss / len(dataloader), rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'train_loss': [], 'test_loss': [], 'test_rmse': [], 'test_mae': []}\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_rmse, test_mae = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_rmse'].append(test_rmse)\n",
    "    history['test_mae'].append(test_mae)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{CONFIG['num_epochs']}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['test_loss'], label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['test_rmse'], label='RMSE')\n",
    "axes[1].plot(history['test_mae'], label='MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Error')\n",
    "axes[1].set_title('Test Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. CONTENT-BASED FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Item Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create item feature matrix (e.g., genres, tags)\n",
    "# Assuming items_df has genre columns\n",
    "\n",
    "# Example: one-hot encode genres\n",
    "# item_features = pd.get_dummies(items_df['genre'])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "# item_similarity = cosine_similarity(item_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 TF-IDF for Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# If items have descriptions\n",
    "# tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "# tfidf_matrix = tfidf.fit_transform(items_df['description'])\n",
    "# item_similarity = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. GENERATING RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Top-K Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recommendations(model, user_idx, item_indices, n=10, device='cpu'):\n",
    "    \"\"\"Get top N recommendations for a user\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    user_tensor = torch.LongTensor([user_idx] * len(item_indices)).to(device)\n",
    "    item_tensor = torch.LongTensor(item_indices).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(user_tensor.unsqueeze(1), item_tensor.unsqueeze(1))\n",
    "    \n",
    "    predictions = predictions.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Get top N\n",
    "    top_indices = np.argsort(predictions)[-n:][::-1]\n",
    "    top_items = [item_indices[i] for i in top_indices]\n",
    "    top_scores = predictions[top_indices]\n",
    "    \n",
    "    return top_items, top_scores\n",
    "\n",
    "# Example: Get recommendations for user 0\n",
    "user_idx = 0\n",
    "all_items = list(range(n_items))\n",
    "\n",
    "# Get items user hasn't rated\n",
    "user_rated_items = set(ratings_filtered[ratings_filtered['user_idx'] == user_idx]['item_idx'].values)\n",
    "unrated_items = [item for item in all_items if item not in user_rated_items]\n",
    "\n",
    "# Get recommendations\n",
    "recommended_items, scores = get_top_n_recommendations(\n",
    "    model, user_idx, unrated_items, n=CONFIG['top_k'], device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nTop {CONFIG['top_k']} Recommendations for User {idx_to_user[user_idx]}:\")\n",
    "for i, (item_idx, score) in enumerate(zip(recommended_items, scores), 1):\n",
    "    item_id = idx_to_item[item_idx]\n",
    "    item_info = items_df[items_df['item_id'] == item_id].iloc[0]\n",
    "    print(f\"{i}. {item_info['title']} (Score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Similar Items (Content-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_items(item_id, item_similarity_matrix, items_df, top_n=10):\n",
    "    \"\"\"Find similar items based on content\"\"\"\n",
    "    item_idx = item_to_idx[item_id]\n",
    "    \n",
    "    # Get similarity scores\n",
    "    similarity_scores = list(enumerate(item_similarity_matrix[item_idx]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top N (excluding itself)\n",
    "    similar_items = similarity_scores[1:top_n+1]\n",
    "    \n",
    "    results = []\n",
    "    for idx, score in similar_items:\n",
    "        item_id_similar = idx_to_item[idx]\n",
    "        item_info = items_df[items_df['item_id'] == item_id_similar].iloc[0]\n",
    "        results.append({\n",
    "            'title': item_info['title'],\n",
    "            'similarity': score\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Precision@K and Recall@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, actual, k=10):\n",
    "    \"\"\"Calculate Precision and Recall at K\"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for user_preds, user_actual in zip(predictions, actual):\n",
    "        # Get top K predictions\n",
    "        top_k = set(user_preds[:k])\n",
    "        relevant = set(user_actual)\n",
    "        \n",
    "        if len(relevant) > 0:\n",
    "            precision = len(top_k & relevant) / k\n",
    "            recall = len(top_k & relevant) / len(relevant)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "    \n",
    "    return np.mean(precisions), np.mean(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(predictions, actual):\n",
    "    \"\"\"Calculate Mean Average Precision\"\"\"\n",
    "    aps = []\n",
    "    \n",
    "    for user_preds, user_actual in zip(predictions, actual):\n",
    "        relevant = set(user_actual)\n",
    "        \n",
    "        if len(relevant) == 0:\n",
    "            continue\n",
    "        \n",
    "        score = 0.0\n",
    "        num_hits = 0.0\n",
    "        \n",
    "        for i, pred in enumerate(user_preds, 1):\n",
    "            if pred in relevant:\n",
    "                num_hits += 1.0\n",
    "                score += num_hits / i\n",
    "        \n",
    "        if len(relevant) > 0:\n",
    "            aps.append(score / len(relevant))\n",
    "    \n",
    "    return np.mean(aps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(predictions, actual, k=10):\n",
    "    \"\"\"Calculate NDCG@K\"\"\"\n",
    "    def dcg_at_k(relevances, k):\n",
    "        relevances = np.asfarray(relevances)[:k]\n",
    "        if relevances.size:\n",
    "            return np.sum(relevances / np.log2(np.arange(2, relevances.size + 2)))\n",
    "        return 0.0\n",
    "    \n",
    "    ndcgs = []\n",
    "    \n",
    "    for user_preds, user_actual in zip(predictions, actual):\n",
    "        relevances = [1 if pred in user_actual else 0 for pred in user_preds[:k]]\n",
    "        \n",
    "        dcg = dcg_at_k(relevances, k)\n",
    "        idcg = dcg_at_k(sorted(relevances, reverse=True), k)\n",
    "        \n",
    "        if idcg > 0:\n",
    "            ndcgs.append(dcg / idcg)\n",
    "    \n",
    "    return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. HYBRID RECOMMENDER SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendations(user_idx, model, item_similarity, alpha=0.5, top_n=10):\n",
    "    \"\"\"\n",
    "    Combine collaborative filtering and content-based filtering\n",
    "    \n",
    "    alpha: weight for collaborative filtering (1-alpha for content-based)\n",
    "    \"\"\"\n",
    "    # Collaborative filtering scores\n",
    "    cf_scores = []\n",
    "    # Content-based scores\n",
    "    cb_scores = []\n",
    "    \n",
    "    # Combine scores\n",
    "    hybrid_scores = alpha * np.array(cf_scores) + (1 - alpha) * np.array(cb_scores)\n",
    "    \n",
    "    # Get top N\n",
    "    top_indices = np.argsort(hybrid_scores)[-top_n:][::-1]\n",
    "    \n",
    "    return top_indices, hybrid_scores[top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. MODEL PERSISTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'user_to_idx': user_to_idx,\n",
    "    'item_to_idx': item_to_idx,\n",
    "    'config': CONFIG\n",
    "}, 'recommendation_model.pth')\n",
    "\n",
    "# Save SVD components\n",
    "np.savez('svd_components.npz', U=U, sigma=sigma, Vt=Vt, mean_rating=mean_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. DEPLOYMENT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderAPI:\n",
    "    \"\"\"Production-ready recommendation API\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, svd_path):\n",
    "        # Load models\n",
    "        checkpoint = torch.load(model_path)\n",
    "        self.model = NeuralCF(n_users, n_items, CONFIG['embedding_dim'])\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.user_to_idx = checkpoint['user_to_idx']\n",
    "        self.item_to_idx = checkpoint['item_to_idx']\n",
    "        \n",
    "        # Load SVD components\n",
    "        svd_data = np.load(svd_path)\n",
    "        self.U = svd_data['U']\n",
    "        self.sigma = svd_data['sigma']\n",
    "        self.Vt = svd_data['Vt']\n",
    "    \n",
    "    def get_recommendations(self, user_id, n=10):\n",
    "        \"\"\"Get top N recommendations for a user\"\"\"\n",
    "        if user_id not in self.user_to_idx:\n",
    "            return self._handle_cold_start(user_id, n)\n",
    "        \n",
    "        user_idx = self.user_to_idx[user_id]\n",
    "        # Generate recommendations\n",
    "        # ...\n",
    "        \n",
    "        return recommended_items\n",
    "    \n",
    "    def _handle_cold_start(self, user_id, n):\n",
    "        \"\"\"Handle new users (cold start problem)\"\"\"\n",
    "        # Return popular items or content-based recommendations\n",
    "        return popular_items[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. CONCLUSIONS & NEXT STEPS\n",
    "\n",
    "## Summary:\n",
    "- Model: Neural Collaborative Filtering\n",
    "- Test RMSE: X.XX\n",
    "- Test MAE: X.XX\n",
    "- Precision@10: X.XX\n",
    "\n",
    "## Next Steps:\n",
    "- [ ] Implement Deep & Cross Network (DCN)\n",
    "- [ ] Add contextual information (time, location)\n",
    "- [ ] Implement multi-armed bandit for exploration\n",
    "- [ ] A/B testing framework\n",
    "- [ ] Real-time recommendation updates\n",
    "- [ ] Implement diversity and serendipity metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
