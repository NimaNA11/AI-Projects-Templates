{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPUTER VISION - OBJECT DETECTION PROJECT TEMPLATE\n",
    "====================================================\n",
    "Use Case: Object Detection, Instance Segmentation, Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PROJECT SETUP & ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Detection specific\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_dir': './data',\n",
    "    'img_size': (640, 640),\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_classes': 10,  # Including background\n",
    "    'num_workers': 4,\n",
    "    'save_dir': './models',\n",
    "    'confidence_threshold': 0.5,\n",
    "    'iou_threshold': 0.5,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "np.random.seed(CONFIG['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DATA LOADING & EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Object Detection\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transforms=None, mode='train'):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Load image paths and annotations\n",
    "        self.images = sorted(list((self.root_dir / 'images' / mode).glob('*.jpg')))\n",
    "        self.annotations = sorted(list((self.root_dir / 'annotations' / mode).glob('*.json')))\n",
    "        \n",
    "        assert len(self.images) == len(self.annotations), \"Mismatch between images and annotations\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = str(self.images[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load annotations\n",
    "        with open(self.annotations[idx], 'r') as f:\n",
    "            annot = json.load(f)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for obj in annot['objects']:\n",
    "            boxes.append(obj['bbox'])  # [xmin, ymin, xmax, ymax]\n",
    "            labels.append(obj['label'])\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx])\n",
    "        }\n",
    "        \n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            image = transformed['image']\n",
    "            target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data for exploration\n",
    "sample_dataset = ObjectDetectionDataset(CONFIG['data_dir'], mode='train')\n",
    "\n",
    "print(f\"Total training samples: {len(sample_dataset)}\")\n",
    "\n",
    "# Visualize sample images with bounding boxes\n",
    "def visualize_sample(dataset, num_samples=4):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        image, target = dataset[i]\n",
    "        \n",
    "        if torch.is_tensor(image):\n",
    "            image = image.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for box, label in zip(target['boxes'], target['labels']):\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                fill=False, color='red', linewidth=2)\n",
    "            axes[i].add_patch(rect)\n",
    "            axes[i].text(x1, y1-5, f'Class {label}', \n",
    "                        color='red', fontsize=10, weight='bold')\n",
    "        \n",
    "        axes[i].set_title(f'Sample {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_sample(sample_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "all_labels = []\n",
    "box_areas = []\n",
    "\n",
    "for i in range(len(sample_dataset)):\n",
    "    _, target = sample_dataset[i]\n",
    "    all_labels.extend(target['labels'].tolist())\n",
    "    \n",
    "    boxes = target['boxes']\n",
    "    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "    box_areas.extend(areas.tolist())\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "pd.Series(all_labels).value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(box_areas, bins=50, edgecolor='black')\n",
    "plt.title('Bounding Box Area Distribution')\n",
    "plt.xlabel('Area (pixels)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DATA AUGMENTATION & PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training augmentations\n",
    "train_transforms = A.Compose([\n",
    "    A.RandomResizedCrop(height=CONFIG['img_size'][0], width=CONFIG['img_size'][1], scale=(0.8, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ColorJitter(p=0.2),\n",
    "    A.Blur(blur_limit=3, p=0.1),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Validation augmentations\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(height=CONFIG['img_size'][0], width=CONFIG['img_size'][1]),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DATALOADER SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for object detection\"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ObjectDetectionDataset(CONFIG['data_dir'], transforms=train_transforms, mode='train')\n",
    "val_dataset = ObjectDetectionDataset(CONFIG['data_dir'], transforms=val_transforms, mode='val')\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    \"\"\"Load Faster R-CNN model with ResNet50 backbone\"\"\"\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Replace classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "        in_features, num_classes\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model(CONFIG['num_classes']).to(device)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Alternative: YOLOv5 (Ultralytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install: pip install ultralytics\n",
    "# from ultralytics import YOLO\n",
    "# \n",
    "# # Load pretrained YOLOv5\n",
    "# model = YOLO('yolov5s.pt')\n",
    "# \n",
    "# # Train\n",
    "# results = model.train(data='data.yaml', epochs=CONFIG['num_epochs'], imgsz=CONFIG['img_size'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': losses.item()})\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"Evaluate model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for images, targets in tqdm(data_loader, desc='Validating'):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        \n",
    "        predictions = model(images)\n",
    "        \n",
    "        all_predictions.extend([{k: v.cpu() for k, v in pred.items()} for pred in predictions])\n",
    "        all_targets.extend([{k: v.cpu() for k, v in t.items()} for t in targets])\n",
    "    \n",
    "    return all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_mAP': []\n",
    "}\n",
    "\n",
    "best_mAP = 0.0\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    predictions, targets = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Calculate mAP (simplified)\n",
    "    # In practice, use COCO evaluation metrics\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    # if val_mAP > best_mAP:\n",
    "    #     best_mAP = val_mAP\n",
    "    #     torch.save(model.state_dict(), f\"{CONFIG['save_dir']}/best_model.pth\")\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between two boxes\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def calculate_map(predictions, targets, iou_threshold=0.5):\n",
    "    \"\"\"Calculate mean Average Precision\"\"\"\n",
    "    # Simplified mAP calculation\n",
    "    # For production, use pycocotools\n",
    "    aps = []\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_boxes = pred['boxes']\n",
    "        pred_scores = pred['scores']\n",
    "        pred_labels = pred['labels']\n",
    "        \n",
    "        target_boxes = target['boxes']\n",
    "        target_labels = target['labels']\n",
    "        \n",
    "        # Filter by confidence threshold\n",
    "        mask = pred_scores > CONFIG['confidence_threshold']\n",
    "        pred_boxes = pred_boxes[mask]\n",
    "        pred_labels = pred_labels[mask]\n",
    "        pred_scores = pred_scores[mask]\n",
    "        \n",
    "        # Calculate AP for this image\n",
    "        # (Simplified implementation)\n",
    "    \n",
    "    return np.mean(aps) if aps else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. VISUALIZATION & INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, num_samples=4):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        image, target = dataset[i]\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = model([image.to(device)])[0]\n",
    "        \n",
    "        # Convert image back to numpy\n",
    "        img = image.permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        \n",
    "        # Draw predictions\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "        labels = prediction['labels'].cpu().numpy()\n",
    "        \n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            if score > CONFIG['confidence_threshold']:\n",
    "                x1, y1, x2, y2 = box\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1,\n",
    "                                    fill=False, color='red', linewidth=2)\n",
    "                axes[i].add_patch(rect)\n",
    "                axes[i].text(x1, y1-5, f'Class {label}: {score:.2f}',\n",
    "                           color='red', fontsize=10, weight='bold',\n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        axes[i].set_title(f'Prediction {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. INFERENCE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image_path, model, device, confidence_threshold=0.5):\n",
    "    \"\"\"Run object detection on a single image\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    transform = val_transforms\n",
    "    transformed = transform(image=image)\n",
    "    input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)[0]\n",
    "    \n",
    "    # Filter by confidence\n",
    "    mask = prediction['scores'] > confidence_threshold\n",
    "    boxes = prediction['boxes'][mask].cpu().numpy()\n",
    "    scores = prediction['scores'][mask].cpu().numpy()\n",
    "    labels = prediction['labels'][mask].cpu().numpy()\n",
    "    \n",
    "    results = {\n",
    "        'boxes': boxes,\n",
    "        'scores': scores,\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    return image, results\n",
    "\n",
    "# Test inference\n",
    "# image, results = detect_objects('test_image.jpg', model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. MODEL EXPORT & DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG\n",
    "}, f\"{CONFIG['save_dir']}/final_model.pth\")\n",
    "\n",
    "# Export to ONNX\n",
    "# dummy_input = torch.randn(1, 3, CONFIG['img_size'][0], CONFIG['img_size'][1]).to(device)\n",
    "# torch.onnx.export(model, dummy_input, \"model.onnx\", \n",
    "#                   export_params=True, opset_version=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. CONCLUSIONS & NEXT STEPS\n",
    "\n",
    "## Summary:\n",
    "- Dataset: X images, Y classes\n",
    "- Architecture: Faster R-CNN with ResNet50\n",
    "- Final mAP: X.XX\n",
    "\n",
    "## Next Steps:\n",
    "- [ ] Experiment with different architectures (YOLOv8, EfficientDet)\n",
    "- [ ] Implement test-time augmentation\n",
    "- [ ] Optimize model with quantization\n",
    "- [ ] Deploy with TensorRT/ONNX Runtime\n",
    "- [ ] Implement real-time video detection\n",
    "- [ ] Add tracking capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
